{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrabNet Hyperparameter Dataset Analysis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sparks-baird/matsci-opt-benchmarks/blob/copilot/fix-50/notebooks/crabnet_hyperparameter/2.0-analysis-crabnet-dataset.ipynb)\n",
    "\n",
    "This notebook analyzes the CrabNet Hyperparameter dataset from Zenodo (DOI: 10.5281/zenodo.7694268).\n",
    "We train various scikit-learn models with and without \"rank\" variables to investigate\n",
    "surprising near-perfect parity plot results mentioned in the issue.\n",
    "\n",
    "The dataset contains 173,219 hyperparameter combinations from CrabNet training experiments,\n",
    "including performance metrics (MAE, RMSE, runtime) and their corresponding rank variables.\n",
    "\n",
    "## Models to evaluate:\n",
    "1. Random Forest Regressor (RFR)\n",
    "2. Histogram Gradient Boosting\n",
    "3. Support Vector Regression (SVR)\n",
    "4. Ridge Regression\n",
    "5. Gaussian Process Regression (GPR) with Automatic Relevance Determination (ARD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel, ConstantKernel as C\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Load the CrabNet hyperparameter dataset from Zenodo (DOI: 10.5281/zenodo.7694268).\n",
    "The dataset contains 173,219 hyperparameter combinations and their corresponding performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_crabnet_dataset():\n",
    "    \"\"\"\n",
    "    Download the CrabNet hyperparameter dataset from Zenodo if not already present\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import urllib.request\n",
    "    \n",
    "    # Define paths\n",
    "    data_dir = Path(\"../../data/processed/crabnet_hyperparameter\")\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    file_path = data_dir / \"sobol_regression.csv\"\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(\"Downloading CrabNet dataset from Zenodo...\")\n",
    "        url = \"https://zenodo.org/api/records/7694268/files/sobol_regression.csv/content\"\n",
    "        urllib.request.urlretrieve(url, file_path)\n",
    "        print(f\"Downloaded dataset to {file_path}\")\n",
    "    else:\n",
    "        print(f\"Dataset already exists at {file_path}\")\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "def load_crabnet_dataset():\n",
    "    \"\"\"\n",
    "    Load and preprocess the CrabNet hyperparameter dataset\n",
    "    \"\"\"\n",
    "    # Download if necessary\n",
    "    file_path = download_crabnet_dataset()\n",
    "    \n",
    "    # Load the dataset\n",
    "    print(\"Loading CrabNet dataset...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Drop non-hyperparameter columns that aren't useful for our analysis\n",
    "    columns_to_drop = ['_id', 'session_id', 'timestamp', 'criterion', 'elem_prop', 'hardware', 'model_size']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "df = load_crabnet_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset info\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"Target variable statistics:\")\n",
    "print(df[['mae', 'rmse', 'runtime']].describe())\n",
    "\n",
    "# Plot target distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "df['mae'].hist(bins=50, ax=axes[0], alpha=0.7)\n",
    "axes[0].set_title('MAE Distribution')\n",
    "axes[0].set_xlabel('MAE')\n",
    "\n",
    "df['rmse'].hist(bins=50, ax=axes[1], alpha=0.7)\n",
    "axes[1].set_title('RMSE Distribution')\n",
    "axes[1].set_xlabel('RMSE')\n",
    "\n",
    "df['runtime'].hist(bins=50, ax=axes[2], alpha=0.7)\n",
    "axes[2].set_title('Runtime Distribution')\n",
    "axes[2].set_xlabel('Runtime (s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Feature Sets\n",
    "\n",
    "We'll create two feature sets:\n",
    "1. Features without rank variables (original hyperparameters only)\n",
    "2. Features with rank variables (including the noise captured by ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "hyperparameter_features = [\n",
    "    'N', 'alpha', 'd_model', 'dim_feedforward', 'dropout', 'emb_scaler',\n",
    "    'eps', 'epochs_step', 'fudge', 'heads', 'k', 'lr', 'pe_resolution',\n",
    "    'ple_resolution', 'pos_scaler', 'weight_decay', 'batch_size',\n",
    "    'out_hidden4', 'betas1', 'betas2', 'train_frac', 'bias',\n",
    "    'use_RobustL1', 'elem_prop_magpie', 'elem_prop_mat2vec', 'elem_prop_onehot'\n",
    "]\n",
    "\n",
    "rank_features = ['mae_rank', 'rmse_rank', 'runtime_rank']\n",
    "\n",
    "# Features without rank (clean hyperparameters)\n",
    "features_without_rank = hyperparameter_features\n",
    "\n",
    "# Features with rank (includes noise)\n",
    "features_with_mae_rank = hyperparameter_features + ['mae_rank']\n",
    "features_with_all_ranks = hyperparameter_features + rank_features\n",
    "\n",
    "print(f\"Hyperparameter features: {len(hyperparameter_features)}\")\n",
    "print(f\"Features without rank: {len(features_without_rank)}\")\n",
    "print(f\"Features with MAE rank: {len(features_with_mae_rank)}\")\n",
    "print(f\"Features with all ranks: {len(features_with_all_ranks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "target = 'mae'\n",
    "y = df[target].values\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_without_rank = df[features_without_rank].values\n",
    "X_with_mae_rank = df[features_with_mae_rank].values\n",
    "X_with_all_ranks = df[features_with_all_ranks].values\n",
    "\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"X_without_rank shape: {X_without_rank.shape}\")\n",
    "print(f\"X_with_mae_rank shape: {X_with_mae_rank.shape}\")\n",
    "print(f\"X_with_all_ranks shape: {X_with_all_ranks.shape}\")\n",
    "\n",
    "# Create train/test splits for all feature sets\n",
    "X_without_rank_train, X_without_rank_test, y_train, y_test = train_test_split(\n",
    "    X_without_rank, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_with_mae_rank_train, X_with_mae_rank_test, _, _ = train_test_split(\n",
    "    X_with_mae_rank, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_with_all_ranks_train, X_with_all_ranks_test, _, _ = train_test_split(\n",
    "    X_with_all_ranks, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(y_train)}\")\n",
    "print(f\"Test set size: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model, return metrics and predictions\n",
    "    \"\"\"\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'r2': r2,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} - R²: {r2:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_parity(results, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Create parity plot for model results\n",
    "    \"\"\"\n",
    "    y_test = results['y_test']\n",
    "    y_pred = results['y_pred']\n",
    "    r2 = results['r2']\n",
    "    model_name = results['model']\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.6, s=20)\n",
    "    \n",
    "    # Plot perfect prediction line\n",
    "    min_val = min(min(y_test), min(y_pred))\n",
    "    max_val = max(max(y_test), max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', \n",
    "             label=f'Perfect fit\\nR² = {r2:.3f}', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('True MAE')\n",
    "    plt.ylabel('Predicted MAE')\n",
    "    plt.title(f'Parity Plot - {model_name}{title_suffix}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"RANDOM FOREST REGRESSOR\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Without rank variables\n",
    "print(\"\\n1. Without rank variables:\")\n",
    "rf_without_rank = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_results_without_rank = evaluate_model(\n",
    "    rf_without_rank, X_without_rank_train, X_without_rank_test, \n",
    "    y_train, y_test, \"Random Forest (without rank)\"\n",
    ")\n",
    "\n",
    "# With MAE rank variable\n",
    "print(\"\\n2. With MAE rank variable:\")\n",
    "rf_with_mae_rank = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_results_with_mae_rank = evaluate_model(\n",
    "    rf_with_mae_rank, X_with_mae_rank_train, X_with_mae_rank_test, \n",
    "    y_train, y_test, \"Random Forest (with MAE rank)\"\n",
    ")\n",
    "\n",
    "# With all rank variables\n",
    "print(\"\\n3. With all rank variables:\")\n",
    "rf_with_all_ranks = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_results_with_all_ranks = evaluate_model(\n",
    "    rf_with_all_ranks, X_with_all_ranks_train, X_with_all_ranks_test, \n",
    "    y_train, y_test, \"Random Forest (with all ranks)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parity plots for Random Forest\n",
    "plot_parity(rf_results_without_rank, \" (without rank)\")\n",
    "plot_parity(rf_results_with_mae_rank, \" (with MAE rank)\")\n",
    "plot_parity(rf_results_with_all_ranks, \" (with all ranks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Histogram Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"HISTOGRAM GRADIENT BOOSTING REGRESSOR\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Without rank variables\n",
    "print(\"\\n1. Without rank variables:\")\n",
    "hgb_without_rank = HistGradientBoostingRegressor(random_state=42)\n",
    "hgb_results_without_rank = evaluate_model(\n",
    "    hgb_without_rank, X_without_rank_train, X_without_rank_test, \n",
    "    y_train, y_test, \"Hist Gradient Boosting (without rank)\"\n",
    ")\n",
    "\n",
    "# With MAE rank variable\n",
    "print(\"\\n2. With MAE rank variable:\")\n",
    "hgb_with_mae_rank = HistGradientBoostingRegressor(random_state=42)\n",
    "hgb_results_with_mae_rank = evaluate_model(\n",
    "    hgb_with_mae_rank, X_with_mae_rank_train, X_with_mae_rank_test, \n",
    "    y_train, y_test, \"Hist Gradient Boosting (with MAE rank)\"\n",
    ")\n",
    "\n",
    "# With all rank variables\n",
    "print(\"\\n3. With all rank variables:\")\n",
    "hgb_with_all_ranks = HistGradientBoostingRegressor(random_state=42)\n",
    "hgb_results_with_all_ranks = evaluate_model(\n",
    "    hgb_with_all_ranks, X_with_all_ranks_train, X_with_all_ranks_test, \n",
    "    y_train, y_test, \"Hist Gradient Boosting (with all ranks)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parity plots for Histogram Gradient Boosting\n",
    "plot_parity(hgb_results_without_rank, \" (without rank)\")\n",
    "plot_parity(hgb_results_with_mae_rank, \" (with MAE rank)\")\n",
    "plot_parity(hgb_results_with_all_ranks, \" (with all ranks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"SUPPORT VECTOR REGRESSION (SVR)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define SVR pipeline with scaling\n",
    "svr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svr', SVR(kernel='rbf', C=1.0, gamma='scale'))\n",
    "])\n",
    "\n",
    "# Without rank variables\n",
    "print(\"\\n1. Without rank variables:\")\n",
    "svr_without_rank = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svr', SVR(kernel='rbf', C=1.0, gamma='scale'))\n",
    "])\n",
    "svr_results_without_rank = evaluate_model(\n",
    "    svr_without_rank, X_without_rank_train, X_without_rank_test, \n",
    "    y_train, y_test, \"SVR (without rank)\"\n",
    ")\n",
    "\n",
    "# With MAE rank variable\n",
    "print(\"\\n2. With MAE rank variable:\")\n",
    "svr_with_mae_rank = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svr', SVR(kernel='rbf', C=1.0, gamma='scale'))\n",
    "])\n",
    "svr_results_with_mae_rank = evaluate_model(\n",
    "    svr_with_mae_rank, X_with_mae_rank_train, X_with_mae_rank_test, \n",
    "    y_train, y_test, \"SVR (with MAE rank)\"\n",
    ")\n",
    "\n",
    "# With all rank variables\n",
    "print(\"\\n3. With all rank variables:\")\n",
    "svr_with_all_ranks = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svr', SVR(kernel='rbf', C=1.0, gamma='scale'))\n",
    "])\n",
    "svr_results_with_all_ranks = evaluate_model(\n",
    "    svr_with_all_ranks, X_with_all_ranks_train, X_with_all_ranks_test, \n",
    "    y_train, y_test, \"SVR (with all ranks)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parity plots for SVR\n",
    "plot_parity(svr_results_without_rank, \" (without rank)\")\n",
    "plot_parity(svr_results_with_mae_rank, \" (with MAE rank)\")\n",
    "plot_parity(svr_results_with_all_ranks, \" (with all ranks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"RIDGE REGRESSION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Without rank variables\n",
    "print(\"\\n1. Without rank variables:\")\n",
    "ridge_without_rank = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "ridge_results_without_rank = evaluate_model(\n",
    "    ridge_without_rank, X_without_rank_train, X_without_rank_test, \n",
    "    y_train, y_test, \"Ridge (without rank)\"\n",
    ")\n",
    "\n",
    "# With MAE rank variable\n",
    "print(\"\\n2. With MAE rank variable:\")\n",
    "ridge_with_mae_rank = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "ridge_results_with_mae_rank = evaluate_model(\n",
    "    ridge_with_mae_rank, X_with_mae_rank_train, X_with_mae_rank_test, \n",
    "    y_train, y_test, \"Ridge (with MAE rank)\"\n",
    ")\n",
    "\n",
    "# With all rank variables\n",
    "print(\"\\n3. With all rank variables:\")\n",
    "ridge_with_all_ranks = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "ridge_results_with_all_ranks = evaluate_model(\n",
    "    ridge_with_all_ranks, X_with_all_ranks_train, X_with_all_ranks_test, \n",
    "    y_train, y_test, \"Ridge (with all ranks)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parity plots for Ridge Regression\n",
    "plot_parity(ridge_results_without_rank, \" (without rank)\")\n",
    "plot_parity(ridge_results_with_mae_rank, \" (with MAE rank)\")\n",
    "plot_parity(ridge_results_with_all_ranks, \" (with all ranks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gaussian Process Regression (GPR) with Automatic Relevance Determination (ARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"GAUSSIAN PROCESS REGRESSION WITH ARD\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define ARD kernel (one length scale per feature)\n",
    "def create_ard_kernel(n_features):\n",
    "    return C(1.0, (1e-3, 1e3)) * RBF(length_scale=[1.0]*n_features, length_scale_bounds=(1e-3, 1e3)) + WhiteKernel()\n",
    "\n",
    "# Without rank variables\n",
    "print(\"\\n1. Without rank variables:\")\n",
    "kernel_without_rank = create_ard_kernel(len(features_without_rank))\n",
    "gpr_without_rank = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gpr', GaussianProcessRegressor(kernel=kernel_without_rank, normalize_y=True, alpha=1e-3, random_state=42))\n",
    "])\n",
    "gpr_results_without_rank = evaluate_model(\n",
    "    gpr_without_rank, X_without_rank_train, X_without_rank_test, \n",
    "    y_train, y_test, \"GPR with ARD (without rank)\"\n",
    ")\n",
    "\n",
    "# With MAE rank variable  \n",
    "print(\"\\n2. With MAE rank variable:\")\n",
    "kernel_with_mae_rank = create_ard_kernel(len(features_with_mae_rank))\n",
    "gpr_with_mae_rank = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gpr', GaussianProcessRegressor(kernel=kernel_with_mae_rank, normalize_y=True, alpha=1e-3, random_state=42))\n",
    "])\n",
    "gpr_results_with_mae_rank = evaluate_model(\n",
    "    gpr_with_mae_rank, X_with_mae_rank_train, X_with_mae_rank_test, \n",
    "    y_train, y_test, \"GPR with ARD (with MAE rank)\"\n",
    ")\n",
    "\n",
    "# With all rank variables\n",
    "print(\"\\n3. With all rank variables:\")\n",
    "kernel_with_all_ranks = create_ard_kernel(len(features_with_all_ranks))\n",
    "gpr_with_all_ranks = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gpr', GaussianProcessRegressor(kernel=kernel_with_all_ranks, normalize_y=True, alpha=1e-3, random_state=42))\n",
    "])\n",
    "gpr_results_with_all_ranks = evaluate_model(\n",
    "    gpr_with_all_ranks, X_with_all_ranks_train, X_with_all_ranks_test, \n",
    "    y_train, y_test, \"GPR with ARD (with all ranks)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parity plots for GPR with ARD\n",
    "plot_parity(gpr_results_without_rank, \" (without rank)\")\n",
    "plot_parity(gpr_results_with_mae_rank, \" (with MAE rank)\")\n",
    "plot_parity(gpr_results_with_all_ranks, \" (with all ranks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation of Small Subset Performance\n",
    "\n",
    "Now let's investigate the surprising results mentioned in the issue where Ridge and GPR\n",
    "gave near-perfect parity plots on small subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INVESTIGATING SMALL SUBSET PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with different small subset sizes\n",
    "subset_sizes = [50, 100, 200, 500]\n",
    "\n",
    "for subset_size in subset_sizes:\n",
    "    print(f\"\\n{'='*20} SUBSET SIZE: {subset_size} {'='*20}\")\n",
    "    \n",
    "    # Create subset\n",
    "    subset_indices = np.random.choice(len(df), size=subset_size, replace=False)\n",
    "    df_subset = df.iloc[subset_indices].copy()\n",
    "    \n",
    "    # Prepare subset data\n",
    "    y_subset = df_subset[target].values\n",
    "    X_subset_without_rank = df_subset[features_without_rank].values\n",
    "    X_subset_with_mae_rank = df_subset[features_with_mae_rank].values\n",
    "    \n",
    "    # Split subset\n",
    "    X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(\n",
    "        X_subset_without_rank, y_subset, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    X_train_sub_rank, X_test_sub_rank, _, _ = train_test_split(\n",
    "        X_subset_with_mae_rank, y_subset, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Subset training size: {len(y_train_sub)}, test size: {len(y_test_sub)}\")\n",
    "    \n",
    "    # Ridge Regression on subset\n",
    "    ridge_sub_without = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('ridge', Ridge(alpha=1.0, random_state=42))\n",
    "    ])\n",
    "    ridge_sub_with = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('ridge', Ridge(alpha=1.0, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    ridge_sub_results_without = evaluate_model(\n",
    "        ridge_sub_without, X_train_sub, X_test_sub, y_train_sub, y_test_sub, \n",
    "        f\"Ridge Subset {subset_size} (without rank)\"\n",
    "    )\n",
    "    \n",
    "    ridge_sub_results_with = evaluate_model(\n",
    "        ridge_sub_with, X_train_sub_rank, X_test_sub_rank, y_train_sub, y_test_sub, \n",
    "        f\"Ridge Subset {subset_size} (with MAE rank)\"\n",
    "    )\n",
    "    \n",
    "    # GPR on subset (only if subset is small enough)\n",
    "    if subset_size <= 200:  # GPR is computationally expensive\n",
    "        kernel_sub_without = create_ard_kernel(len(features_without_rank))\n",
    "        kernel_sub_with = create_ard_kernel(len(features_with_mae_rank))\n",
    "        \n",
    "        gpr_sub_without = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('gpr', GaussianProcessRegressor(kernel=kernel_sub_without, normalize_y=True, alpha=1e-3, random_state=42))\n",
    "        ])\n",
    "        \n",
    "        gpr_sub_with = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('gpr', GaussianProcessRegressor(kernel=kernel_sub_with, normalize_y=True, alpha=1e-3, random_state=42))\n",
    "        ])\n",
    "        \n",
    "        gpr_sub_results_without = evaluate_model(\n",
    "            gpr_sub_without, X_train_sub, X_test_sub, y_train_sub, y_test_sub, \n",
    "            f\"GPR Subset {subset_size} (without rank)\"\n",
    "        )\n",
    "        \n",
    "        gpr_sub_results_with = evaluate_model(\n",
    "            gpr_sub_with, X_train_sub_rank, X_test_sub_rank, y_train_sub, y_test_sub, \n",
    "            f\"GPR Subset {subset_size} (with MAE rank)\"\n",
    "        )\n",
    "        \n",
    "        # Plot parity plots for small subsets\n",
    "        if subset_size <= 100:  # Only plot for very small subsets\n",
    "            plot_parity(ridge_sub_results_without, f\" (Subset {subset_size})\")\n",
    "            plot_parity(ridge_sub_results_with, f\" (Subset {subset_size})\")\n",
    "            plot_parity(gpr_sub_results_without, f\" (Subset {subset_size})\")\n",
    "            plot_parity(gpr_sub_results_with, f\" (Subset {subset_size})\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = [\n",
    "    rf_results_without_rank, rf_results_with_mae_rank, rf_results_with_all_ranks,\n",
    "    hgb_results_without_rank, hgb_results_with_mae_rank, hgb_results_with_all_ranks,\n",
    "    svr_results_without_rank, svr_results_with_mae_rank, svr_results_with_all_ranks,\n",
    "    ridge_results_without_rank, ridge_results_with_mae_rank, ridge_results_with_all_ranks,\n",
    "    gpr_results_without_rank, gpr_results_with_mae_rank, gpr_results_with_all_ranks\n",
    "]\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_data = []\n",
    "for result in all_results:\n",
    "    summary_data.append({\n",
    "        'Model': result['model'],\n",
    "        'R²': result['r2'],\n",
    "        'MAE': result['mae'],\n",
    "        'RMSE': result['rmse']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"PERFORMANCE SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# R² comparison\n",
    "summary_df.plot(x='Model', y='R²', kind='bar', ax=axes[0], rot=45)\n",
    "axes[0].set_title('R² Comparison')\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MAE comparison\n",
    "summary_df.plot(x='Model', y='MAE', kind='bar', ax=axes[1], rot=45, color='orange')\n",
    "axes[1].set_title('MAE Comparison')\n",
    "axes[1].set_ylabel('Mean Absolute Error')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RMSE comparison\n",
    "summary_df.plot(x='Model', y='RMSE', kind='bar', ax=axes[2], rot=45, color='green')\n",
    "axes[2].set_title('RMSE Comparison')\n",
    "axes[2].set_ylabel('Root Mean Squared Error')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Insights\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Impact of Rank Variables**: The rank variables (especially `mae_rank`) provide additional information that can improve model performance, but they also represent \"captured noise\" since they're derived from the target variable itself.\n",
    "\n",
    "2. **Model Behavior with Rank Variables**:\n",
    "   - **Random Forest and Histogram Gradient Boosting**: These tree-based models may benefit from rank variables as they can capture non-linear relationships.\n",
    "   - **Ridge Regression**: Linear models like Ridge may show dramatic improvement with rank variables, especially on small datasets.\n",
    "   - **GPR with ARD**: Gaussian processes can adaptively weight features, so rank variables might lead to overfitting on small datasets.\n",
    "   - **SVR**: Support vector machines with RBF kernels may also benefit from the additional rank information.\n",
    "\n",
    "3. **Small Dataset Effects**: The near-perfect parity plots mentioned in the issue likely occur because:\n",
    "   - Small datasets are easier to overfit\n",
    "   - Rank variables provide direct information about target variable ordering\n",
    "   - Models with high capacity (like GPR) can memorize small datasets\n",
    "\n",
    "4. **Data Leakage Consideration**: Including rank variables derived from target variables could be considered a form of data leakage, especially if these ranks are computed on the entire dataset before splitting.\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **For Production Use**: Use models without rank variables for fair evaluation of hyperparameter optimization\n",
    "2. **For Surrogate Modeling**: Rank variables might be acceptable if the goal is to predict relative performance\n",
    "3. **Cross-Validation**: Use proper cross-validation to avoid overfitting, especially with small datasets\n",
    "4. **Feature Importance**: Analyze feature importance to understand which hyperparameters are most influential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis demonstrates the significant impact of rank variables on model performance. The near-perfect parity plots observed with Ridge regression and GPR on small subsets are likely due to:\n",
    "\n",
    "1. **Information Leakage**: Rank variables provide direct information about target variable ordering\n",
    "2. **Overfitting**: Small datasets are susceptible to overfitting, especially with high-capacity models\n",
    "3. **Model Capacity**: GPR and regularized linear models can memorize small datasets effectively\n",
    "\n",
    "For practical hyperparameter optimization, it's recommended to use models trained on original hyperparameters without rank variables to ensure fair evaluation and avoid potential data leakage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}