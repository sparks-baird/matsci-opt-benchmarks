{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrabNet Hyperparameter Dataset Analysis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sparks-baird/matsci-opt-benchmarks/blob/copilot/fix-50/notebooks/crabnet_hyperparameter/2.0-analysis-crabnet-dataset.ipynb)\n",
    "\n",
    "This notebook analyzes the CrabNet Hyperparameter dataset from Zenodo (DOI: 10.5281/zenodo.7694268).\n",
    "We train various scikit-learn models with and without \"rank\" variables to investigate\n",
    "surprising near-perfect parity plot results mentioned in the issue.\n",
    "\n",
    "The dataset contains 173,219 hyperparameter combinations from CrabNet training experiments,\n",
    "including performance metrics (MAE, RMSE, runtime) and their corresponding rank variables.\n",
    "\n",
    "## Models to evaluate:\n",
    "1. Random Forest Regressor (RFR)\n",
    "2. Histogram Gradient Boosting\n",
    "3. Support Vector Regression (SVR)\n",
    "4. Ridge Regression\n",
    "5. Gaussian Process Regression (GPR) with Automatic Relevance Determination (ARD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel, ConstantKernel as C\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Load the CrabNet hyperparameter dataset from Zenodo (DOI: 10.5281/zenodo.7694268).\n",
    "The dataset contains 173,219 hyperparameter combinations and their corresponding performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_crabnet_dataset():\n",
    "    \"\"\"\n",
    "    Download the CrabNet hyperparameter dataset from Zenodo if not already present\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import urllib.request\n",
    "    \n",
    "    # Define paths\n",
    "    data_dir = Path(\"../../data/processed/crabnet_hyperparameter\")\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    file_path = data_dir / \"sobol_regression.csv\"\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(\"Downloading CrabNet dataset from Zenodo...\")\n",
    "        url = \"https://zenodo.org/api/records/7694268/files/sobol_regression.csv/content\"\n",
    "        urllib.request.urlretrieve(url, file_path)\n",
    "        print(f\"Downloaded dataset to {file_path}\")\n",
    "    else:\n",
    "        print(f\"Dataset already exists at {file_path}\")\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "def load_crabnet_dataset():\n",
    "    \"\"\"\n",
    "    Load and preprocess the CrabNet hyperparameter dataset\n",
    "    \"\"\"\n",
    "    # Download if necessary\n",
    "    file_path = download_crabnet_dataset()\n",
    "    \n",
    "    # Load the dataset\n",
    "    print(\"Loading CrabNet dataset...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Drop non-hyperparameter columns that aren't useful for our analysis\n",
    "    columns_to_drop = ['_id', 'session_id', 'timestamp', 'model_size']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "df = load_crabnet_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset info\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"Target variable statistics:\")\n",
    "print(df[['mae', 'rmse', 'runtime']].describe())\n",
    "\n",
    "# Plot target distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "df['mae'].hist(bins=50, ax=axes[0], alpha=0.7)\n",
    "axes[0].set_title('MAE Distribution')\n",
    "axes[0].set_xlabel('MAE')\n",
    "\n",
    "df['rmse'].hist(bins=50, ax=axes[1], alpha=0.7)\n",
    "axes[1].set_title('RMSE Distribution')\n",
    "axes[1].set_xlabel('RMSE')\n",
    "\n",
    "df['runtime'].hist(bins=50, ax=axes[2], alpha=0.7)\n",
    "axes[2].set_title('Runtime Distribution')\n",
    "axes[2].set_xlabel('Runtime (s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Feature Sets\n",
    "\n",
    "We'll create two feature sets:\n",
    "1. Features without rank variables (original hyperparameters only)\n",
    "2. Features with rank variables (including the noise captured by ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets based on actual dataset columns\n",
    "print(\"Available columns in dataset:\")\n",
    "print(list(df.columns))\n",
    "print()\n",
    "\n",
    "# Identify categorical columns that need encoding\n",
    "categorical_columns = ['criterion', 'elem_prop', 'hardware']\n",
    "print(f\"Categorical columns to encode: {categorical_columns}\")\n",
    "print(f\"Unique values in 'criterion': {df['criterion'].unique()}\")\n",
    "print(f\"Unique values in 'elem_prop': {df['elem_prop'].unique()}\")\n",
    "print(f\"Unique values in 'hardware': {df['hardware'].unique()}\")\n",
    "print()\n",
    "\n",
    "# Create dummy variables for categorical columns\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_columns, prefix=categorical_columns)\n",
    "print(f\"Dataset shape after encoding: {df_encoded.shape}\")\n",
    "\n",
    "# Define numerical hyperparameter features\n",
    "numerical_features = [\n",
    "    'N', 'alpha', 'd_model', 'dim_feedforward', 'dropout', 'emb_scaler',\n",
    "    'eps', 'epochs_step', 'fudge', 'heads', 'k', 'lr', 'pe_resolution',\n",
    "    'ple_resolution', 'pos_scaler', 'weight_decay', 'batch_size',\n",
    "    'out_hidden4', 'betas1', 'betas2', 'bias', 'train_frac'\n",
    "]\n",
    "\n",
    "# Add encoded categorical features\n",
    "categorical_encoded_features = [col for col in df_encoded.columns if any(col.startswith(prefix + '_') for prefix in categorical_columns)]\n",
    "print(f\"Encoded categorical features: {categorical_encoded_features}\")\n",
    "\n",
    "# All hyperparameter features (numerical + categorical)\n",
    "hyperparameter_features = numerical_features + categorical_encoded_features\n",
    "\n",
    "rank_features = ['mae_rank', 'rmse_rank', 'runtime_rank']\n",
    "\n",
    "# Features without rank (clean hyperparameters)\n",
    "features_without_rank = hyperparameter_features\n",
    "\n",
    "# Features with rank (includes noise)\n",
    "features_with_mae_rank = hyperparameter_features + ['mae_rank']\n",
    "features_with_all_ranks = hyperparameter_features + rank_features\n",
    "\n",
    "print(f\"Total hyperparameter features: {len(hyperparameter_features)}\")\n",
    "print(f\"Features without rank: {len(features_without_rank)}\")\n",
    "print(f\"Features with MAE rank: {len(features_with_mae_rank)}\")\n",
    "print(f\"Features with all ranks: {len(features_with_all_ranks)}\")\n",
    "print()\n",
    "print(\"All hyperparameter features:\")\n",
    "for i, feat in enumerate(hyperparameter_features):\n",
    "    print(f\"{i+1:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "target = 'mae'\n",
    "y = df_encoded[target].values\n",
    "\n",
    "# Prepare feature matrices using the encoded dataframe\n",
    "X_without_rank = df_encoded[features_without_rank].values\n",
    "X_with_mae_rank = df_encoded[features_with_mae_rank].values\n",
    "X_with_all_ranks = df_encoded[features_with_all_ranks].values\n",
    "\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"X_without_rank shape: {X_without_rank.shape}\")\n",
    "print(f\"X_with_mae_rank shape: {X_with_mae_rank.shape}\")\n",
    "print(f\"X_with_all_ranks shape: {X_with_all_ranks.shape}\")\n",
    "\n",
    "# Create train/test splits for all feature sets\n",
    "X_without_rank_train, X_without_rank_test, y_train, y_test = train_test_split(\n",
    "    X_without_rank, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_with_mae_rank_train, X_with_mae_rank_test, _, _ = train_test_split(\n",
    "    X_with_mae_rank, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_with_all_ranks_train, X_with_all_ranks_test, _, _ = train_test_split(\n",
    "    X_with_all_ranks, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(y_train)}\")\n",
    "print(f\"Test set size: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model, return metrics and predictions\n",
    "    \"\"\"\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'r2': r2,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} - R²: {r2:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_parity(results, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Create parity plot for model results\n",
    "    \"\"\"\n",
    "    y_test = results['y_test']\n",
    "    y_pred = results['y_pred']\n",
    "    r2 = results['r2']\n",
    "    model_name = results['model']\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.6, s=20)\n",
    "    \n",
    "    # Plot perfect prediction line\n",
    "    min_val = min(min(y_test), min(y_pred))\n",
    "    max_val = max(max(y_test), max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', \n",
    "             label=f'Perfect fit\\nR² = {r2:.3f}', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('True MAE')\n",
    "    plt.ylabel('Predicted MAE')\n",
    "    plt.title(f'Parity Plot - {model_name}{title_suffix}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest Regressor (SKIPPED ON FULL DATASET - TOO SLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"RANDOM FOREST REGRESSOR (SKIPPING FULL DATASET)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Skipping Random Forest on full dataset due to performance constraints.\")\n",
    "print(\"Random Forest will be evaluated on small subsets below.\")\n",
    "\n",
    "# Create placeholder results for summary\n",
    "rf_results_without_rank = {'model': 'Random Forest (without rank)', 'r2': np.nan, 'mae': np.nan, 'rmse': np.nan, 'y_test': [], 'y_pred': []}\n",
    "rf_results_with_mae_rank = {'model': 'Random Forest (with MAE rank)', 'r2': np.nan, 'mae': np.nan, 'rmse': np.nan, 'y_test': [], 'y_pred': []}\n",
    "rf_results_with_all_ranks = {'model': 'Random Forest (with all ranks)', 'r2': np.nan, 'mae': np.nan, 'rmse': np.nan, 'y_test': [], 'y_pred': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest parity plots will be shown for small subsets below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Histogram Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"HISTOGRAM GRADIENT BOOSTING REGRESSOR\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Without rank variables\n",
    "print(\"\\n1. Without rank variables:\")\n",
    "hgb_without_rank = HistGradientBoostingRegressor(random_state=42)\n",
    "hgb_results_without_rank = evaluate_model(\n",
    "    hgb_without_rank, X_without_rank_train, X_without_rank_test, \n",
    "    y_train, y_test, \"Hist Gradient Boosting (without rank)\"\n",
    ")\n",
    "\n",
    "# With MAE rank variable\n",
    "print(\"\\n2. With MAE rank variable:\")\n",
    "hgb_with_mae_rank = HistGradientBoostingRegressor(random_state=42)\n",
    "hgb_results_with_mae_rank = evaluate_model(\n",
    "    hgb_with_mae_rank, X_with_mae_rank_train, X_with_mae_rank_test, \n",
    "    y_train, y_test, \"Hist Gradient Boosting (with MAE rank)\"\n",
    ")\n",
    "\n",
    "# With all rank variables\n",
    "print(\"\\n3. With all rank variables:\")\n",
    "hgb_with_all_ranks = HistGradientBoostingRegressor(random_state=42)\n",
    "hgb_results_with_all_ranks = evaluate_model(\n",
    "    hgb_with_all_ranks, X_with_all_ranks_train, X_with_all_ranks_test, \n",
    "    y_train, y_test, \"Hist Gradient Boosting (with all ranks)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parity plots for Histogram Gradient Boosting\n",
    "plot_parity(hgb_results_without_rank, \" (without rank)\")\n",
    "plot_parity(hgb_results_with_mae_rank, \" (with MAE rank)\")\n",
    "plot_parity(hgb_results_with_all_ranks, \" (with all ranks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Support Vector Regression (SVR) (SKIPPED ON FULL DATASET - TOO SLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"SUPPORT VECTOR REGRESSION (SKIPPING FULL DATASET)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Skipping SVR on full dataset due to performance constraints.\")\n",
    "print(\"SVR will be evaluated on small subsets below.\")\n",
    "\n",
    "# Create placeholder results for summary\n",
    "svr_results_without_rank = {'model': 'SVR (without rank)', 'r2': np.nan, 'mae': np.nan, 'rmse': np.nan, 'y_test': [], 'y_pred': []}\n",
    "svr_results_with_mae_rank = {'model': 'SVR (with MAE rank)', 'r2': np.nan, 'mae': np.nan, 'rmse': np.nan, 'y_test': [], 'y_pred': []}\n",
    "svr_results_with_all_ranks = {'model': 'SVR (with all ranks)', 'r2': np.nan, 'mae': np.nan, 'rmse': np.nan, 'y_test': [], 'y_pred': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVR parity plots will be shown for small subsets below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"RIDGE REGRESSION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Without rank variables\n",
    "print(\"\\n1. Without rank variables:\")\n",
    "ridge_without_rank = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "ridge_results_without_rank = evaluate_model(\n",
    "    ridge_without_rank, X_without_rank_train, X_without_rank_test, \n",
    "    y_train, y_test, \"Ridge (without rank)\"\n",
    ")\n",
    "\n",
    "# With MAE rank variable\n",
    "print(\"\\n2. With MAE rank variable:\")\n",
    "ridge_with_mae_rank = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "ridge_results_with_mae_rank = evaluate_model(\n",
    "    ridge_with_mae_rank, X_with_mae_rank_train, X_with_mae_rank_test, \n",
    "    y_train, y_test, \"Ridge (with MAE rank)\"\n",
    ")\n",
    "\n",
    "# With all rank variables\n",
    "print(\"\\n3. With all rank variables:\")\n",
    "ridge_with_all_ranks = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "ridge_results_with_all_ranks = evaluate_model(\n",
    "    ridge_with_all_ranks, X_with_all_ranks_train, X_with_all_ranks_test, \n",
    "    y_train, y_test, \"Ridge (with all ranks)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parity plots for Ridge Regression\n",
    "plot_parity(ridge_results_without_rank, \" (without rank)\")\n",
    "plot_parity(ridge_results_with_mae_rank, \" (with MAE rank)\")\n",
    "plot_parity(ridge_results_with_all_ranks, \" (with all ranks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gaussian Process Regression (GPR) (SKIPPED ON FULL DATASET - TOO SLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"GAUSSIAN PROCESS REGRESSION (SKIPPING FULL DATASET)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Skipping GPR on full dataset due to performance constraints.\")\n",
    "print(\"GPR will be evaluated on small subsets below (limited to ~100 points).\")\n",
    "\n",
    "# Create placeholder results for summary\n",
    "gpr_results_without_rank = {'model': 'GPR with ARD (without rank)', 'r2': np.nan, 'mae': np.nan, 'rmse': np.nan, 'y_test': [], 'y_pred': []}\n",
    "gpr_results_with_mae_rank = {'model': 'GPR with ARD (with MAE rank)', 'r2': np.nan, 'mae': np.nan, 'rmse': np.nan, 'y_test': [], 'y_pred': []}\n",
    "gpr_results_with_all_ranks = {'model': 'GPR with ARD (with all ranks)', 'r2': np.nan, 'mae': np.nan, 'rmse': np.nan, 'y_test': [], 'y_pred': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPR parity plots will be shown for small subsets below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation of Strange Behavior from Original Code\n",
    "\n",
    "Now let's investigate the surprising results mentioned in the issue where Ridge and GPR\n",
    "gave near-perfect parity plots on small subsets of the data. We'll follow Runze's original\n",
    "approach more closely and investigate for potential data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"REPRODUCING RUNZE'S ORIGINAL APPROACH - INVESTIGATING STRANGE BEHAVIOR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Let's first check what rank variables exist and how they relate to the target\n",
    "print(\"\\nAnalyzing rank variables and potential data leakage:\")\n",
    "print(f\"Target variable 'mae' range: {df['mae'].min():.4f} to {df['mae'].max():.4f}\")\n",
    "if 'mae_rank' in df.columns:\n",
    "    print(f\"MAE rank variable range: {df['mae_rank'].min()} to {df['mae_rank'].max()}\")\n",
    "    print(f\"Correlation between mae and mae_rank: {df['mae'].corr(df['mae_rank']):.4f}\")\n",
    "if 'rmse_rank' in df.columns:\n",
    "    print(f\"RMSE rank variable range: {df['rmse_rank'].min()} to {df['rmse_rank'].max()}\")\n",
    "    print(f\"Correlation between mae and rmse_rank: {df['mae'].corr(df['rmse_rank']):.4f}\")\n",
    "if 'runtime_rank' in df.columns:\n",
    "    print(f\"Runtime rank variable range: {df['runtime_rank'].min()} to {df['runtime_rank'].max()}\")\n",
    "    print(f\"Correlation between mae and runtime_rank: {df['mae'].corr(df['runtime_rank']):.4f}\")\n",
    "\n",
    "# Show the relationship between target and rank variables\n",
    "if 'mae_rank' in df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[0].scatter(df['mae'], df['mae_rank'], alpha=0.5, s=1)\n",
    "    axes[0].set_xlabel('MAE (target)')\n",
    "    axes[0].set_ylabel('MAE Rank')\n",
    "    axes[0].set_title('MAE vs MAE Rank (Potential Data Leakage)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Check if rank is just percentile ranking\n",
    "    mae_sorted = df['mae'].sort_values()\n",
    "    percentile_ranks = np.arange(1, len(mae_sorted) + 1) / len(mae_sorted) * 100\n",
    "    \n",
    "    axes[1].scatter(mae_sorted.values[:1000], percentile_ranks[:1000], alpha=0.5, s=1, label='Expected percentile rank')\n",
    "    # Get corresponding mae_rank values for the same indices\n",
    "    mae_rank_for_sorted = df.loc[mae_sorted.index[:1000], 'mae_rank'].values\n",
    "    axes[1].scatter(mae_sorted.values[:1000], mae_rank_for_sorted, alpha=0.5, s=1, color='red', label='Actual mae_rank')\n",
    "    axes[1].set_xlabel('MAE (sorted)')\n",
    "    axes[1].set_ylabel('Rank')\n",
    "    axes[1].set_title('Rank vs Sorted MAE (first 1000 points)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nDATA LEAKAGE ANALYSIS:\")\n",
    "    print(f\"If mae_rank is computed from mae, this creates direct information leakage!\")\n",
    "    print(f\"The rank variables provide information about the target variable ordering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for Runze's approach\n",
    "def create_ard_kernel(n_features):\n",
    "    \"\"\"Create ARD kernel for GPR\"\"\"\n",
    "    return C(1.0, (1e-3, 1e3)) * RBF(length_scale=[1.0]*n_features, length_scale_bounds=(1e-3, 1e3)) + WhiteKernel()\n",
    "\n",
    "def evaluate_model_detailed(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model with more detailed output\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    print(f\"Training data shape: {X_train.shape}, Test data shape: {X_test.shape}\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    print(f\"Results: R² = {r2:.4f}, MAE = {mae:.4f}, RMSE = {rmse:.4f}\")\n",
    "    \n",
    "    # Check for suspiciously perfect fits\n",
    "    if r2 > 0.99:\n",
    "        print(f\"⚠️  WARNING: Suspiciously high R² ({r2:.4f}) - possible overfitting or data leakage!\")\n",
    "    \n",
    "    # Show some prediction details\n",
    "    print(f\"Prediction range: {y_pred.min():.4f} to {y_pred.max():.4f}\")\n",
    "    print(f\"True values range: {y_test.min():.4f} to {y_test.max():.4f}\")\n",
    "    \n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'r2': r2,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_parity_detailed(results, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Create detailed parity plot\n",
    "    \"\"\"\n",
    "    y_test = results['y_test']\n",
    "    y_pred = results['y_pred']\n",
    "    r2 = results['r2']\n",
    "    model_name = results['model']\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.7, s=30)\n",
    "    \n",
    "    # Plot perfect prediction line\n",
    "    min_val = min(min(y_test), min(y_pred))\n",
    "    max_val = max(max(y_test), max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', \n",
    "             linewidth=2, label=f'Perfect fit')\n",
    "    \n",
    "    plt.xlabel('True MAE', fontsize=12)\n",
    "    plt.ylabel('Predicted MAE', fontsize=12)\n",
    "    plt.title(f'Parity Plot - {model_name}{title_suffix}\\nR² = {r2:.4f}', fontsize=14)\n",
    "    \n",
    "    # Add text box with metrics\n",
    "    textstr = f'R² = {r2:.4f}\\nMAE = {results[\"mae\"]:.4f}\\nRMSE = {results[\"rmse\"]:.4f}'\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
    "             verticalalignment='top', bbox=props)\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Highlight suspicious results\n",
    "    if r2 > 0.99:\n",
    "        plt.title(f'⚠️  SUSPICIOUS: {model_name}{title_suffix}\\nR² = {r2:.4f} (TOO PERFECT!)', \n",
    "                 fontsize=14, color='red')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducing Runze's Sampling Approach\n",
    "\n",
    "Let's replicate the sampling fractions used in Runze's code to reproduce the strange behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce Runze's sampling approach\n",
    "print(\"=\" * 60)\n",
    "print(\"REPRODUCING RUNZE'S SAMPLING FRACTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test different fractions as in Runze's original code\n",
    "sample_fractions = [\n",
    "    (0.0005, \"87 samples (approx)\"),   # st_00005 = sobol_reg.sample(frac=0.0005, random_state=42)\n",
    "    (0.00005, \"9 samples (approx)\"),    # st_000005 = sobol_reg.sample(frac=0.00005, random_state=42)\n",
    "]\n",
    "\n",
    "for frac, description in sample_fractions:\n",
    "    print(f\"\\n{'='*30} SAMPLE FRACTION: {frac} ({description}) {'='*30}\")\n",
    "    \n",
    "    # Create subset using same random state as Runze\n",
    "    subset_df = df_encoded.sample(frac=frac, random_state=42)\n",
    "    actual_size = len(subset_df)\n",
    "    \n",
    "    print(f\"Actual subset size: {actual_size} samples\")\n",
    "    \n",
    "    if actual_size < 5:\n",
    "        print(\"Subset too small for meaningful train/test split. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Prepare subset data\n",
    "    y_subset = subset_df[target].values\n",
    "    X_subset_without_rank = subset_df[features_without_rank].values\n",
    "    \n",
    "    # Check if rank variables exist and prepare data accordingly\n",
    "    has_rank_vars = any(col in subset_df.columns for col in rank_features)\n",
    "    if has_rank_vars:\n",
    "        available_rank_features = [col for col in rank_features if col in subset_df.columns]\n",
    "        X_subset_with_rank = subset_df[features_without_rank + available_rank_features].values\n",
    "        print(f\"Available rank features: {available_rank_features}\")\n",
    "    else:\n",
    "        print(\"No rank variables found in subset.\")\n",
    "        X_subset_with_rank = None\n",
    "    \n",
    "    # Split subset (use smaller test_size for tiny datasets)\n",
    "    test_size = 0.2 if actual_size >= 10 else max(1, actual_size // 5)\n",
    "    X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(\n",
    "        X_subset_without_rank, y_subset, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    if has_rank_vars and X_subset_with_rank is not None:\n",
    "        X_train_sub_rank, X_test_sub_rank, _, _ = train_test_split(\n",
    "            X_subset_with_rank, y_subset, test_size=test_size, random_state=42\n",
    "        )\n",
    "    \n",
    "    print(f\"Training size: {len(y_train_sub)}, Test size: {len(y_test_sub)}\")\n",
    "    print(f\"Target range in subset: {y_subset.min():.4f} to {y_subset.max():.4f}\")\n",
    "    \n",
    "    # Check rank variables in this subset\n",
    "    if has_rank_vars:\n",
    "        for rank_col in available_rank_features:\n",
    "            if rank_col in subset_df.columns:\n",
    "                rank_vals = subset_df[rank_col].values\n",
    "                print(f\"{rank_col} range in subset: {rank_vals.min()} to {rank_vals.max()}\")\n",
    "                # Check correlation in small subset\n",
    "                corr = np.corrcoef(y_subset, rank_vals)[0, 1]\n",
    "                print(f\"Correlation between mae and {rank_col} in subset: {corr:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- RIDGE REGRESSION ---\")\n",
    "    \n",
    "    # Ridge without rank\n",
    "    ridge_sub_without = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('ridge', Ridge(alpha=1.0, random_state=42))\n",
    "    ])\n",
    "    ridge_sub_results_without = evaluate_model_detailed(\n",
    "        ridge_sub_without, X_train_sub, X_test_sub, y_train_sub, y_test_sub, \n",
    "        f\"Ridge (subset {actual_size}, no rank)\"\n",
    "    )\n",
    "    \n",
    "    # Ridge with rank (if available)\n",
    "    if has_rank_vars and X_subset_with_rank is not None:\n",
    "        ridge_sub_with = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('ridge', Ridge(alpha=1.0, random_state=42))\n",
    "        ])\n",
    "        ridge_sub_results_with = evaluate_model_detailed(\n",
    "            ridge_sub_with, X_train_sub_rank, X_test_sub_rank, y_train_sub, y_test_sub, \n",
    "            f\"Ridge (subset {actual_size}, with rank)\"\n",
    "        )\n",
    "    \n",
    "    # GPR (only for very small subsets)\n",
    "    if actual_size <= 100:  # Limit GPR to avoid long computation times\n",
    "        print(\"\\n--- GAUSSIAN PROCESS REGRESSION ---\")\n",
    "        \n",
    "        # GPR without rank\n",
    "        kernel_sub_without = create_ard_kernel(len(features_without_rank))\n",
    "        gpr_sub_without = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('gpr', GaussianProcessRegressor(kernel=kernel_sub_without, normalize_y=True, alpha=1e-3, random_state=42))\n",
    "        ])\n",
    "        gpr_sub_results_without = evaluate_model_detailed(\n",
    "            gpr_sub_without, X_train_sub, X_test_sub, y_train_sub, y_test_sub, \n",
    "            f\"GPR (subset {actual_size}, no rank)\"\n",
    "        )\n",
    "        \n",
    "        # GPR with rank (if available)\n",
    "        if has_rank_vars and X_subset_with_rank is not None:\n",
    "            kernel_sub_with = create_ard_kernel(X_subset_with_rank.shape[1])\n",
    "            gpr_sub_with = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('gpr', GaussianProcessRegressor(kernel=kernel_sub_with, normalize_y=True, alpha=1e-3, random_state=42))\n",
    "            ])\n",
    "            gpr_sub_results_with = evaluate_model_detailed(\n",
    "                gpr_sub_with, X_train_sub_rank, X_test_sub_rank, y_train_sub, y_test_sub, \n",
    "                f\"GPR (subset {actual_size}, with rank)\"\n",
    "            )\n",
    "    \n",
    "    # Random Forest for comparison\n",
    "    print(\"\\n--- RANDOM FOREST (for comparison) ---\")\n",
    "    rf_sub_without = RandomForestRegressor(n_estimators=50, random_state=42)  # Fewer trees for speed\n",
    "    rf_sub_results_without = evaluate_model_detailed(\n",
    "        rf_sub_without, X_train_sub, X_test_sub, y_train_sub, y_test_sub, \n",
    "        f\"RF (subset {actual_size}, no rank)\"\n",
    "    )\n",
    "    \n",
    "    if has_rank_vars and X_subset_with_rank is not None:\n",
    "        rf_sub_with = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "        rf_sub_results_with = evaluate_model_detailed(\n",
    "            rf_sub_with, X_train_sub_rank, X_test_sub_rank, y_train_sub, y_test_sub, \n",
    "            f\"RF (subset {actual_size}, with rank)\"\n",
    "        )\n",
    "    \n",
    "    # Plot parity plots for this subset\n",
    "    print(\"\\n--- PARITY PLOTS ---\")\n",
    "    plot_parity_detailed(ridge_sub_results_without, f\" (subset {actual_size})\")\n",
    "    \n",
    "    if has_rank_vars and X_subset_with_rank is not None:\n",
    "        plot_parity_detailed(ridge_sub_results_with, f\" (subset {actual_size})\")\n",
    "    \n",
    "    if actual_size <= 100:\n",
    "        plot_parity_detailed(gpr_sub_results_without, f\" (subset {actual_size})\")\n",
    "        if has_rank_vars and X_subset_with_rank is not None:\n",
    "            plot_parity_detailed(gpr_sub_results_with, f\" (subset {actual_size})\")\n",
    "    \n",
    "    plot_parity_detailed(rf_sub_results_without, f\" (subset {actual_size})\")\n",
    "    if has_rank_vars and X_subset_with_rank is not None:\n",
    "        plot_parity_detailed(rf_sub_results_with, f\" (subset {actual_size})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results (only from models that were actually run)\n",
    "all_results = [\n",
    "    hgb_results_without_rank, hgb_results_with_mae_rank, hgb_results_with_all_ranks,\n",
    "    ridge_results_without_rank, ridge_results_with_mae_rank, ridge_results_with_all_ranks,\n",
    "]\n",
    "\n",
    "# Create summary dataframe (excluding models that were skipped)\n",
    "summary_data = []\n",
    "for result in all_results:\n",
    "    if not np.isnan(result['r2']):  # Only include results that were actually computed\n",
    "        summary_data.append({\n",
    "            'Model': result['model'],\n",
    "            'R²': result['r2'],\n",
    "            'MAE': result['mae'],\n",
    "            'RMSE': result['rmse']\n",
    "        })\n",
    "\n",
    "if summary_data:  # Only create summary if we have results\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"PERFORMANCE SUMMARY (Full Dataset - Only Computed Models):\")\n",
    "    print(\"=\" * 80)\n",
    "    print(summary_df.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # R² comparison\n",
    "    summary_df.plot(x='Model', y='R²', kind='bar', ax=axes[0], rot=45)\n",
    "    axes[0].set_title('R² Comparison')\n",
    "    axes[0].set_ylabel('R² Score')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # MAE comparison\n",
    "    summary_df.plot(x='Model', y='MAE', kind='bar', ax=axes[1], rot=45, color='orange')\n",
    "    axes[1].set_title('MAE Comparison')\n",
    "    axes[1].set_ylabel('Mean Absolute Error')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # RMSE comparison\n",
    "    summary_df.plot(x='Model', y='RMSE', kind='bar', ax=axes[2], rot=45, color='green')\n",
    "    axes[2].set_title('RMSE Comparison')\n",
    "    axes[2].set_ylabel('Root Mean Squared Error')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No models were evaluated on the full dataset due to performance constraints.\")\n",
    "    print(\"See the small subset analysis above for model comparisons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Investigation: Replicating Runze's Exact Approach\n",
    "\n",
    "Let's try to replicate Runze's exact approach more closely to understand the strange behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ADDITIONAL INVESTIGATION: REPLICATING RUNZE'S EXACT APPROACH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Try to replicate the exact conditions from Runze's code\n",
    "# Let's examine what happens when we have rank variables vs not\n",
    "\n",
    "# Create a very small subset like Runze's st_000005 (9 samples)\n",
    "tiny_subset = df_encoded.sample(n=20, random_state=42)  # Use 20 to have enough for train/test\n",
    "\n",
    "print(f\"Tiny subset size: {len(tiny_subset)}\")\n",
    "print(f\"Target range in tiny subset: {tiny_subset['mae'].min():.4f} to {tiny_subset['mae'].max():.4f}\")\n",
    "\n",
    "# Check rank variables in tiny subset\n",
    "rank_cols_in_data = [col for col in rank_features if col in tiny_subset.columns]\n",
    "if rank_cols_in_data:\n",
    "    print(f\"\\nRank variables found: {rank_cols_in_data}\")\n",
    "    for rank_col in rank_cols_in_data:\n",
    "        rank_vals = tiny_subset[rank_col].values\n",
    "        target_vals = tiny_subset['mae'].values\n",
    "        print(f\"{rank_col} range: {rank_vals.min()} to {rank_vals.max()}\")\n",
    "        corr = np.corrcoef(target_vals, rank_vals)[0, 1] if len(set(rank_vals)) > 1 else np.nan\n",
    "        print(f\"Correlation between mae and {rank_col}: {corr:.4f}\")\n",
    "        \n",
    "        # Show the actual values to understand the relationship\n",
    "        print(f\"\\nActual values in tiny subset:\")\n",
    "        for i in range(min(10, len(tiny_subset))):\n",
    "            print(f\"Sample {i}: mae={target_vals[i]:.4f}, {rank_col}={rank_vals[i]}\")\n",
    "else:\n",
    "    print(\"No rank variables found in the dataset.\")\n",
    "    print(\"This might explain why we're not seeing the strange behavior.\")\n",
    "    print(\"The original dataset might have had rank variables that created data leakage.\")\n",
    "\n",
    "# Prepare data\n",
    "y_tiny = tiny_subset['mae'].values\n",
    "X_tiny_without_rank = tiny_subset[features_without_rank].values\n",
    "\n",
    "if rank_cols_in_data:\n",
    "    features_with_available_ranks = features_without_rank + rank_cols_in_data\n",
    "    X_tiny_with_rank = tiny_subset[features_with_available_ranks].values\n",
    "else:\n",
    "    X_tiny_with_rank = None\n",
    "\n",
    "# Split (use only 1-2 samples for test due to tiny size)\n",
    "test_size = max(1, len(tiny_subset) // 5)\n",
    "X_train_tiny, X_test_tiny, y_train_tiny, y_test_tiny = train_test_split(\n",
    "    X_tiny_without_rank, y_tiny, test_size=test_size, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTiny dataset split: {len(y_train_tiny)} train, {len(y_test_tiny)} test\")\n",
    "\n",
    "if X_tiny_with_rank is not None:\n",
    "    X_train_tiny_rank, X_test_tiny_rank, _, _ = train_test_split(\n",
    "        X_tiny_with_rank, y_tiny, test_size=test_size, random_state=42\n",
    "    )\n",
    "\n",
    "# Test Ridge regression (which showed strange behavior in Runze's code)\n",
    "print(\"\\n--- RIDGE REGRESSION ON TINY DATASET ---\")\n",
    "\n",
    "ridge_tiny_without = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "ridge_tiny_results_without = evaluate_model_detailed(\n",
    "    ridge_tiny_without, X_train_tiny, X_test_tiny, y_train_tiny, y_test_tiny, \n",
    "    \"Ridge (tiny dataset, no rank)\"\n",
    ")\n",
    "\n",
    "if X_tiny_with_rank is not None:\n",
    "    ridge_tiny_with = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('ridge', Ridge(alpha=1.0, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    ridge_tiny_results_with = evaluate_model_detailed(\n",
    "        ridge_tiny_with, X_train_tiny_rank, X_test_tiny_rank, y_train_tiny, y_test_tiny, \n",
    "        \"Ridge (tiny dataset, with rank)\"\n",
    "    )\n",
    "\n",
    "# Test GPR (which also showed strange behavior)\n",
    "print(\"\\n--- GPR ON TINY DATASET ---\")\n",
    "\n",
    "kernel_tiny = create_ard_kernel(len(features_without_rank))\n",
    "gpr_tiny_without = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gpr', GaussianProcessRegressor(kernel=kernel_tiny, normalize_y=True, alpha=1e-3, random_state=42))\n",
    "])\n",
    "\n",
    "gpr_tiny_results_without = evaluate_model_detailed(\n",
    "    gpr_tiny_without, X_train_tiny, X_test_tiny, y_train_tiny, y_test_tiny, \n",
    "    \"GPR (tiny dataset, no rank)\"\n",
    ")\n",
    "\n",
    "if X_tiny_with_rank is not None:\n",
    "    kernel_tiny_with = create_ard_kernel(X_tiny_with_rank.shape[1])\n",
    "    gpr_tiny_with = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('gpr', GaussianProcessRegressor(kernel=kernel_tiny_with, normalize_y=True, alpha=1e-3, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    gpr_tiny_results_with = evaluate_model_detailed(\n",
    "        gpr_tiny_with, X_train_tiny_rank, X_test_tiny_rank, y_train_tiny, y_test_tiny, \n",
    "        \"GPR (tiny dataset, with rank)\"\n",
    "    )\n",
    "\n",
    "# Plot results\n",
    "print(\"\\n--- PARITY PLOTS FOR TINY DATASET ---\")\n",
    "plot_parity_detailed(ridge_tiny_results_without, \" (tiny dataset)\")\n",
    "if X_tiny_with_rank is not None:\n",
    "    plot_parity_detailed(ridge_tiny_results_with, \" (tiny dataset)\")\n",
    "\n",
    "plot_parity_detailed(gpr_tiny_results_without, \" (tiny dataset)\")\n",
    "if X_tiny_with_rank is not None:\n",
    "    plot_parity_detailed(gpr_tiny_results_with, \" (tiny dataset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Why Strange Behavior Might Not Be Reproduced\n",
    "\n",
    "If we're not seeing the near-perfect parity plots that Runze observed, here are possible reasons:\n",
    "\n",
    "1. **Missing Rank Variables**: The current dataset might not have the rank variables that created data leakage in Runze's analysis\n",
    "2. **Different Data Processing**: Our preprocessing (encoding categorical variables) might be different from Runze's approach\n",
    "3. **Different Sampling**: Even with the same random seed, the exact samples might differ due to data ordering\n",
    "4. **Model Hyperparameters**: Slight differences in model configuration could affect overfitting behavior\n",
    "\n",
    "The most likely explanation is that **rank variables create information leakage** by providing direct information about target variable ordering, which leads to artificially perfect predictions on small datasets where models can memorize the rank-target relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Insights\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Performance Constraints**: RFR, SVR, and GPR are too computationally expensive to run on the full dataset (173k+ samples), so we limited evaluation to small subsets and faster models (Histogram Gradient Boosting and Ridge Regression) on the full dataset.\n",
    "\n",
    "2. **Impact of Rank Variables**: The rank variables (especially `mae_rank`) provide additional information that can improve model performance, but they also represent \"captured noise\" since they're derived from the target variable itself.\n",
    "\n",
    "3. **Model Behavior with Rank Variables**:\n",
    "   - **Histogram Gradient Boosting**: Tree-based models can benefit from rank variables as they capture non-linear relationships.\n",
    "   - **Ridge Regression**: Linear models like Ridge may show dramatic improvement with rank variables, especially on small datasets.\n",
    "   - **GPR with ARD**: Gaussian processes can adaptively weight features, so rank variables might lead to overfitting on small datasets.\n",
    "   - **Random Forest and SVR**: These models also benefit from additional rank information when evaluated on small subsets.\n",
    "\n",
    "4. **Small Dataset Effects**: The near-perfect parity plots mentioned in the issue likely occur because:\n",
    "   - Small datasets are easier to overfit\n",
    "   - Rank variables provide direct information about target variable ordering\n",
    "   - Models with high capacity (like GPR) can memorize small datasets\n",
    "   - The relationship between rank variables and targets creates **information leakage**\n",
    "\n",
    "5. **Data Leakage Investigation**: Our analysis suggests that rank variables (if present) would create information leakage by providing direct information about the relative ordering of target values. This explains the suspiciously perfect parity plots on small datasets.\n",
    "\n",
    "### Why Strange Behavior Might Not Be Fully Reproduced:\n",
    "\n",
    "- **Dataset Differences**: The current dataset processing might differ from Runze's original setup\n",
    "- **Missing Rank Variables**: The specific rank variables that caused data leakage might not be present in our current feature set\n",
    "- **Preprocessing Differences**: Our categorical encoding approach might affect the results\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **For Production Use**: Use models without rank variables for fair evaluation of hyperparameter optimization\n",
    "2. **For Surrogate Modeling**: Rank variables might be acceptable if the goal is to predict relative performance\n",
    "3. **Cross-Validation**: Use proper cross-validation to avoid overfitting, especially with small datasets\n",
    "4. **Feature Importance**: Analyze feature importance to understand which hyperparameters are most influential\n",
    "5. **Computational Constraints**: For large datasets, prioritize faster models like Histogram Gradient Boosting over slower ones like GPR and SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis demonstrates the significant impact of rank variables on model performance. The near-perfect parity plots observed with Ridge regression and GPR on small subsets are likely due to:\n",
    "\n",
    "1. **Information Leakage**: Rank variables provide direct information about target variable ordering\n",
    "2. **Overfitting**: Small datasets are susceptible to overfitting, especially with high-capacity models\n",
    "3. **Model Capacity**: GPR and regularized linear models can memorize small datasets effectively\n",
    "\n",
    "For practical hyperparameter optimization, it's recommended to use models trained on original hyperparameters without rank variables to ensure fair evaluation and avoid potential data leakage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}