{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Packing Surrogate Model\n",
    "\n",
    "Here, we train a surrogate model for the particle packing simulations. We capture the\n",
    "presence of failed simulations, the packing fractions for two different algorithms, and\n",
    "the corresponding runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from os import path\n",
    "import json\n",
    "\n",
    "# attempted use of skl2onnx to convert to onnx failing due to protobuf error\n",
    "# https://github.com/onnx/onnx/issues/4469\n",
    "\n",
    "# from skl2onnx import convert_sklearn\n",
    "# from skl2onnx.common.data_types import FloatTensorType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dummy = False\n",
    "\n",
    "task_name = \"particle_packing\"\n",
    "data_dir = path.join(\"..\", \"..\", \"data\", \"processed\", task_name)\n",
    "model_dir = path.join(\"..\", \"..\", \"models\", task_name)\n",
    "\n",
    "if dummy:\n",
    "    model_dir = path.join(model_dir, \"dummy\")\n",
    "\n",
    "cv_model_dir = path.join(model_dir, \"cv\")\n",
    "\n",
    "Path(model_dir).mkdir(exist_ok=True, parents=True) # technically redundant\n",
    "Path(cv_model_dir).mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobol_filter = pd.read_csv(path.join(data_dir, \"sobol_probability_filter.csv\"))\n",
    "sobol_reg = pd.read_csv(path.join(data_dir, \"sobol_regression.csv\"))\n",
    "\n",
    "if dummy:\n",
    "    data_dir = path.join(data_dir, \"dummy\")\n",
    "    sobol_filter = sobol_filter.head(100)\n",
    "    sobol_reg = sobol_reg.head(100)\n",
    "\n",
    "Path(data_dir).mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define f(x) to calc mae scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group kfold split for cv; addressing data leakage by using groups\n",
    "def rfr_group_mae(X_array, y_array, group_array, model_name_stem, random_state=13):\n",
    "    kf = GroupKFold(n_splits=5)\n",
    "    mae_scores = []\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        kf.split(X_array, y_array, group_array)\n",
    "    ):\n",
    "        X_train, X_test = X_array[train_index], X_array[test_index]\n",
    "        y_train, y_test = y_array[train_index], y_array[test_index]\n",
    "        y_test = y_test.tolist()\n",
    "\n",
    "        model = RandomForestRegressor(random_state=random_state)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test).tolist()\n",
    "\n",
    "        y_preds.append(y_pred)\n",
    "        y_trues.append(y_test)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mae_scores.append(mae)\n",
    "        # save model as .pkl with compression\n",
    "        # https://stackoverflow.com/a/47062881/13697228\n",
    "        joblib.dump(model, f\"{model_name_stem}_{i}.pkl\", compress=7)\n",
    "    avg_mae = np.mean(mae_scores)\n",
    "    std_mae = np.std(mae_scores)\n",
    "    print(f\"MAE for {path.basename(model_name_stem)}: {avg_mae:.4f} +/- {std_mae:.4f}\")\n",
    "    results = {\"mae\": mae_scores, \"y_pred\": y_preds, \"y_true\": y_trues}\n",
    "    return results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_features = [\n",
    "    \"mu1\",\n",
    "    \"mu2\",\n",
    "    \"mu3\",\n",
    "    \"std1\",\n",
    "    \"std2\",\n",
    "    \"std3\",\n",
    "    \"comp1\",\n",
    "    \"comp2\",\n",
    "    \"comp3\",\n",
    "    \"num_particles\",\n",
    "    \"safety_factor\",\n",
    "]\n",
    "\n",
    "fba_isna_prob_features = common_features\n",
    "ls_isna_prob_features = common_features\n",
    "fba_features = common_features + [\"fba_rank\"]\n",
    "ls_features = common_features + [\"ls_rank\"]\n",
    "fba_time_s_features = common_features + [\"fba_time_s_rank\"]\n",
    "ls_time_s_features = common_features + [\"ls_time_s_rank\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Filter\n",
    "### fba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create groups for Probablity filter using features of sobol_filter\n",
    "sobol_filter_group = (\n",
    "    sobol_filter[common_features]\n",
    "    .round(6)\n",
    "    .apply(lambda row: \"_\".join(row.values.astype(str)), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for spf_fba_isna_prob: 0.1015 +/- 0.0169\n"
     ]
    }
   ],
   "source": [
    "## Create a GroupKFold cross-validation iterator\n",
    "\n",
    "X_array_fba_isna_prob = sobol_filter[fba_isna_prob_features].to_numpy()\n",
    "y_array_fba_isna_prob = sobol_filter[[\"fba_isna_prob\"]].to_numpy().ravel()\n",
    "\n",
    "## This is the trained model on As a function of mu1_div_mu3, mu2_div_mu3, std1, std2,\n",
    "## std3, comp1, comp2, num_particles, safety_factor\n",
    "## label data = fba_isna_prob\n",
    "\n",
    "fba_isna_model_stem = path.join(cv_model_dir, \"spf_fba_isna_prob\")\n",
    "fba_isna_results = rfr_group_mae(\n",
    "    X_array_fba_isna_prob,\n",
    "    y_array_fba_isna_prob,\n",
    "    sobol_filter_group,\n",
    "    fba_isna_model_stem,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test loading the pickled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [0.05908556 0.36824183 0.06840663 0.12804622 0.3107619 ]\n",
      "1: [0.0494958  0.39304696 0.03669553 0.02937675 0.2591664 ]\n",
      "2: [0.07344444 0.38519824 0.05323016 0.05344444 0.3379304 ]\n",
      "3: [0.01448739 0.12500866 0.02310823 0.01679552 0.30702381]\n",
      "4: [0.08705602 0.37276912 0.1100177  0.08304945 0.38245743]\n"
     ]
    }
   ],
   "source": [
    "test_data = X_array_fba_isna_prob[:5]\n",
    "for i in range(5):\n",
    "    model = joblib.load(f\"{fba_isna_model_stem}_{i}.pkl\")\n",
    "    print(f\"{i}: {model.predict(test_data)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for spf_ls_isna_prob: 0.1751 +/- 0.0283\n"
     ]
    }
   ],
   "source": [
    "sobolPF_ls_isna_prob = sobol_filter[ls_isna_prob_features]\n",
    "ls_isna_prob = sobol_filter[[\"ls_isna_prob\"]]\n",
    "\n",
    "X_array_ls_isna_prob = sobolPF_ls_isna_prob.to_numpy()\n",
    "y_array_ls_isna_prob = ls_isna_prob.to_numpy().ravel()\n",
    "\n",
    "ls_isna_model_stem = path.join(cv_model_dir, \"spf_ls_isna_prob\")\n",
    "ls_isna_results = rfr_group_mae(\n",
    "    X_array_ls_isna_prob, y_array_ls_isna_prob, sobol_filter_group, ls_isna_model_stem\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packing Fraction Models\n",
    "### fba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for sobol_reg_fba: 0.0102 +/- 0.0023\n"
     ]
    }
   ],
   "source": [
    "sobol_reg_fba = sobol_reg.dropna(subset=[\"fba\"])\n",
    "X_array_fba = sobol_reg_fba[fba_features].to_numpy()\n",
    "y_array_fba = sobol_reg_fba[\"fba\"].to_numpy().ravel()\n",
    "\n",
    "## create group for sobol regression fba features\n",
    "sobol_reg_fba_group = (\n",
    "    sobol_reg_fba[common_features]\n",
    "    .round(6)\n",
    "    .apply(lambda row: \"_\".join(row.values.astype(str)), axis=1)\n",
    ")\n",
    "\n",
    "\n",
    "## GroupKFold split for cv; using groups\n",
    "fba_model_stem = path.join(cv_model_dir, \"sobol_reg_fba\")\n",
    "fba_results = rfr_group_mae(\n",
    "    X_array_fba, y_array_fba, sobol_reg_fba_group, fba_model_stem\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for sobol_reg_ls: 0.0236 +/- 0.0037\n"
     ]
    }
   ],
   "source": [
    "sobol_reg_ls = sobol_reg.dropna(subset=[\"ls\"])\n",
    "X_array_ls = sobol_reg_ls[ls_features].to_numpy()\n",
    "y_array_ls = sobol_reg_ls[\"ls\"].to_numpy().ravel()\n",
    "\n",
    "## create group for sobol regression ls features\n",
    "sobol_reg_ls_group = (\n",
    "    sobol_reg_ls[common_features]\n",
    "    .round(6)\n",
    "    .apply(lambda row: \"_\".join(row.values.astype(str)), axis=1)\n",
    ")\n",
    "\n",
    "## GroupKFold split for cv; using groups\n",
    "ls_model_path = path.join(cv_model_dir, \"sobol_reg_ls\")\n",
    "ls_results = rfr_group_mae(X_array_ls, y_array_ls, sobol_reg_ls_group, ls_model_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Models\n",
    "No NaNs in the time values.\n",
    "### fba_time_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for sobol_reg_fba_time_s: 0.0338 +/- 0.0126\n"
     ]
    }
   ],
   "source": [
    "## create fba_time_s dataframe to use for groups\n",
    "fba_time_s_df = sobol_reg[fba_time_s_features]\n",
    "\n",
    "X_array_fba_time_s = sobol_reg[fba_time_s_features].to_numpy()\n",
    "fba_time_s = sobol_reg[[\"fba_time_s\"]]\n",
    "y_array_fba_time_s = fba_time_s.to_numpy().ravel()\n",
    "\n",
    "\n",
    "##create groups for fba_time_s GroupKFOld split\n",
    "sobol_reg_fba_time_s_group = (\n",
    "    fba_time_s_df[common_features]\n",
    "    .round(6)\n",
    "    .apply(lambda row: \"_\".join(row.values.astype(str)), axis=1)\n",
    ")\n",
    "\n",
    "fba_time_s_model_stem = path.join(cv_model_dir, \"sobol_reg_fba_time_s\")\n",
    "fba_time_s_results = rfr_group_mae(\n",
    "    X_array_fba_time_s,\n",
    "    y_array_fba_time_s,\n",
    "    sobol_reg_fba_time_s_group,\n",
    "    fba_time_s_model_stem,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ls_time_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for sobol_reg_ls_time_s: 2.9251 +/- 1.1571\n"
     ]
    }
   ],
   "source": [
    "##create df for ls_time_s\n",
    "ls_time_s_df = sobol_reg[ls_time_s_features]\n",
    "\n",
    "##create arrays for model\n",
    "X_array_ls_time_s = sobol_reg[ls_time_s_features].to_numpy()\n",
    "ls_time_s = sobol_reg[[\"ls_time_s\"]]\n",
    "y_array_ls_time_s = ls_time_s.to_numpy().ravel()\n",
    "\n",
    "\n",
    "##create groups for fba_time_s GroupKFOld split\n",
    "sobol_reg_ls_time_s_group = (\n",
    "    ls_time_s_df[common_features]\n",
    "    .round(6)\n",
    "    .apply(lambda row: \"_\".join(row.values.astype(str)), axis=1)\n",
    ")\n",
    "\n",
    "\n",
    "ls_time_s_model_stem = path.join(cv_model_dir, \"sobol_reg_ls_time_s\")\n",
    "ls_time_s_results = rfr_group_mae(\n",
    "    X_array_ls_time_s,\n",
    "    y_array_ls_time_s,\n",
    "    sobol_reg_ls_time_s_group,\n",
    "    ls_time_s_model_stem,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder where is the data and what is it saving\n",
    "main_results = {\n",
    "    \"fba_isna_prob\": fba_isna_results,\n",
    "    \"ls_isna_prob\": ls_isna_results,\n",
    "    \"fba\": fba_results,\n",
    "    \"ls\": ls_results,\n",
    "    \"fba_time_s\": fba_time_s_results,\n",
    "    \"ls_time_s\": ls_time_s_results,\n",
    "}\n",
    "with open(path.join(data_dir, \"model_metadata.json\"), \"w\") as f:\n",
    "    json.dump(main_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = {\n",
    "    \"fba_isna_prob\": fba_isna_model_stem,\n",
    "    \"ls_isna_prob\": ls_isna_model_stem,\n",
    "    \"fba\": fba_model_stem,\n",
    "    \"ls\": ls_model_path,\n",
    "    \"fba_time_s\": fba_time_s_model_stem,\n",
    "    \"ls_time_s\": ls_time_s_model_stem,\n",
    "}\n",
    "\n",
    "for i in range(5):\n",
    "    models = {}\n",
    "    for key, model_path in model_paths.items():\n",
    "        models[key] = joblib.load(f\"{model_path}_{i}.pkl\")\n",
    "\n",
    "    with open(path.join(cv_model_dir, f\"cross_validation_models_{i}.pkl\"), \"wb\") as f:\n",
    "        joblib.dump(models, f, compress=7)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production models (full training data)\n",
    "Six keys in the dictionary, each key is a value of a label, and its value pair is the trained model.\n",
    "This trained model is stored in the models folder with the pickle file name \"trained_model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save(\n",
    "    spf_feat_array,\n",
    "    sr_feat_array,\n",
    "    spf_labels_array,\n",
    "    sr_labels_array,\n",
    "    spf_label_names,\n",
    "    sr_label_names,\n",
    "):\n",
    "    models = {}\n",
    "\n",
    "    for X1, y1, name1 in zip(spf_feat_array, spf_labels_array, spf_label_names):\n",
    "        print(f\"X1 spf shape: {X1.shape}, Y1 spf shape: {y1.shape}\")\n",
    "        model = RandomForestRegressor(random_state=13)\n",
    "        model.fit(X1, y1)\n",
    "        models[name1] = model\n",
    "\n",
    "    for X2, y2, name2 in zip(sr_feat_array, sr_labels_array, sr_label_names):\n",
    "        print(f\"X2 sr shape: {X2.shape}, Y2 sr shape: {y2.shape}\")\n",
    "        model = RandomForestRegressor(random_state=13)\n",
    "        model.fit(X2, y2)\n",
    "        models[name2] = model\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 spf shape: (100, 11), Y1 spf shape: (100,)\n",
      "X1 spf shape: (100, 11), Y1 spf shape: (100,)\n",
      "X2 sr shape: (78, 12), Y2 sr shape: (78,)\n",
      "X2 sr shape: (68, 12), Y2 sr shape: (68,)\n",
      "X2 sr shape: (100, 12), Y2 sr shape: (100,)\n",
      "X2 sr shape: (100, 12), Y2 sr shape: (100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['..\\\\..\\\\models\\\\particle_packing\\\\dummy\\\\surrogate_models.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of x_arrays, y_arrays, and target_names\n",
    "sobol_prob_filter_arrays = [X_array_fba_isna_prob, X_array_ls_isna_prob]\n",
    "sobol_prob_filter_labels = [y_array_fba_isna_prob, y_array_ls_isna_prob]\n",
    "sobol_filter_target_names = [\"fba_isna_prob\", \"ls_isna_prob\"]\n",
    "\n",
    "# List of x_arrays, y_arrays, and target_names\n",
    "sobol_reg_x_arrays = [X_array_fba, X_array_ls, X_array_fba_time_s, X_array_ls_time_s]\n",
    "sobol_reg_labels = [y_array_fba, y_array_ls, y_array_fba_time_s, y_array_ls_time_s]\n",
    "sobol_reg_target_names = [\"fba\", \"ls\", \"fba_time_s\", \"ls_time_s\"]\n",
    "\n",
    "# Train and save the model on all the data\n",
    "models = train_and_save(\n",
    "    sobol_prob_filter_arrays,\n",
    "    sobol_reg_x_arrays,\n",
    "    sobol_prob_filter_labels,\n",
    "    sobol_reg_labels,\n",
    "    sobol_filter_target_names,\n",
    "    sobol_reg_target_names,\n",
    ")\n",
    "\n",
    "joblib.dump(models, path.join(model_dir, \"surrogate_models.pkl\"), compress=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matsci-opt-benchmarks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01883adffc5ff99e80740fdb2688c7d7f1b5220f2274814f600fbe3b3887f376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
