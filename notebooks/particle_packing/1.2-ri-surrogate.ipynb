{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Packing Surrogate Model\n",
    "\n",
    "Here, we train a surrogate model for the particle packing simulations. We capture the\n",
    "presence of failed simulations, the packing fractions for two different algorithms, and\n",
    "the corresponding runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from os import path\n",
    "import json\n",
    "\n",
    "# attempted use of skl2onnx to convert to onnx failing due to protobuf error\n",
    "# https://github.com/onnx/onnx/issues/4469\n",
    "\n",
    "# from skl2onnx import convert_sklearn\n",
    "# from skl2onnx.common.data_types import FloatTensorType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = False\n",
    "\n",
    "data_dir = path.join(\"..\", \"..\", \"data\", \"processed\", \"particle_packing\")\n",
    "model_dir = path.join(\"..\", \"..\", \"models\", \"particle_packing\")\n",
    "\n",
    "if dummy:\n",
    "    model_dir = path.join(model_dir, \"dummy\")\n",
    "\n",
    "cv_model_dir = path.join(model_dir, \"cv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobol_filter = pd.read_csv(path.join(data_dir, \"sobol_probability_filter.csv\"))\n",
    "sobol_reg = pd.read_csv(path.join(data_dir, \"sobol_regression.csv\"))\n",
    "\n",
    "if dummy:\n",
    "    sobol_filter = sobol_filter.head(100)\n",
    "    sobol_reg = sobol_reg.head(100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define f(x) to calc mae scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group kfold split for cv; addressing data leakage by using groups\n",
    "def rfr_group_mae(X_array, y_array, group_array, model_name_stem, random_state=13):\n",
    "    kf = GroupKFold(n_splits=5)\n",
    "    mae_scores = []\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        kf.split(X_array, y_array, group_array)\n",
    "    ):\n",
    "        X_train, X_test = X_array[train_index], X_array[test_index]\n",
    "        y_train, y_test = y_array[train_index], y_array[test_index]\n",
    "        y_test = y_test.tolist()\n",
    "\n",
    "        model = RandomForestRegressor(random_state=random_state)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test).tolist()\n",
    "\n",
    "        y_preds.append(y_pred)\n",
    "        y_trues.append(y_test)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mae_scores.append(mae)\n",
    "        # save model as .pkl with compression\n",
    "        # https://stackoverflow.com/a/47062881/13697228\n",
    "        joblib.dump(model, f\"{model_name_stem}_{i}.pkl\", compress=3)\n",
    "    avg_mae = np.mean(mae_scores)\n",
    "    std_mae = np.std(mae_scores)\n",
    "    print(f\"MAE for {path.basename(model_name_stem)}: {avg_mae:.4f} +/- {std_mae:.4f}\")\n",
    "    results = {\"mae\": mae_scores, \"y_pred\": y_preds, \"y_true\": y_trues}\n",
    "    return results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_features = [\n",
    "    \"mu1_div_mu3\",\n",
    "    \"mu2_div_mu3\",\n",
    "    \"std1\",\n",
    "    \"std2\",\n",
    "    \"std3\",\n",
    "    \"comp1\",\n",
    "    \"comp2\",\n",
    "    \"num_particles\",\n",
    "    \"safety_factor\",\n",
    "]\n",
    "\n",
    "fba_isna_prob_features = common_features\n",
    "ls_isna_prob_features = common_features\n",
    "fba_features = common_features + [\"fba_rank\"]\n",
    "ls_features = common_features + [\"ls_rank\"]\n",
    "fba_time_s_features = common_features + [\"fba_time_s_rank\"]\n",
    "ls_time_s_features = common_features + [\"ls_time_s_rank\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Filter\n",
    "### fba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create groups for Probablity filter using features of sobol_filter\n",
    "sobol_filter_groups = np.array(\n",
    "    [\n",
    "        f\"{mu1},{mu2},{std1},{std2},{std3},{comp1},{comp2},{num_part},{safety_fac}\"\n",
    "        for mu1, mu2, std1, std2, std3, comp1, comp2, num_part, safety_fac in zip(\n",
    "            sobol_filter[\"mu1_div_mu3\"],\n",
    "            sobol_filter[\"mu2_div_mu3\"],\n",
    "            sobol_filter[\"std1\"],\n",
    "            sobol_filter[\"std2\"],\n",
    "            sobol_filter[\"std3\"],\n",
    "            sobol_filter[\"comp1\"],\n",
    "            sobol_filter[\"comp2\"],\n",
    "            sobol_filter[\"num_particles\"],\n",
    "            sobol_filter[\"safety_factor\"],\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for spf_fba_isna_prob: 0.0490 +/- 0.0004\n"
     ]
    }
   ],
   "source": [
    "## Create a GroupKFold cross-validation iterator\n",
    "\n",
    "X_array_fba_isna_prob = sobol_filter[fba_isna_prob_features].to_numpy()\n",
    "y_array_fba_isna_prob = sobol_filter[[\"fba_isna_prob\"]].to_numpy().ravel()\n",
    "\n",
    "## This is the trained model on As a function of mu1_div_mu3, mu2_div_mu3, std1, std2,\n",
    "## std3, comp1, comp2, num_particles, safety_factor\n",
    "## label data = fba_isna_prob\n",
    "\n",
    "fba_isna_model_stem = path.join(cv_model_dir, \"spf_fba_isna_prob\")\n",
    "fba_isna_results = rfr_group_mae(\n",
    "    X_array_fba_isna_prob,\n",
    "    y_array_fba_isna_prob,\n",
    "    sobol_filter_groups,\n",
    "    fba_isna_model_stem,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test loading the pickled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [0.15800239 0.         0.00554545 0.06155267 0.1096148 ]\n",
      "1: [0.16549081 0.         0.01188034 0.06469852 0.16106888]\n",
      "2: [0.16487179 0.         0.0109864  0.05001859 0.08853974]\n",
      "3: [0.16212352 0.00166667 0.03510412 0.06572747 0.10368279]\n",
      "4: [0.15947577 0.00524242 0.01400913 0.05925311 0.08923135]\n"
     ]
    }
   ],
   "source": [
    "test_data = X_array_fba_isna_prob[:5]\n",
    "for i in range(5):\n",
    "    model = joblib.load(f\"{fba_isna_model_stem}_{i}.pkl\")\n",
    "    print(f\"{i}: {model.predict(test_data)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for spf_ls_isna_prob: 0.0912 +/- 0.0004\n"
     ]
    }
   ],
   "source": [
    "sobolPF_ls_isna_prob = sobol_filter[ls_isna_prob_features]\n",
    "ls_isna_prob = sobol_filter[[\"ls_isna_prob\"]]\n",
    "\n",
    "X_array_ls_isna_prob = sobolPF_ls_isna_prob.to_numpy()\n",
    "y_array_ls_isna_prob = ls_isna_prob.to_numpy().ravel()\n",
    "\n",
    "ls_isna_model_stem = path.join(cv_model_dir, \"spf_ls_isna_prob\")\n",
    "ls_isna_results = rfr_group_mae(\n",
    "    X_array_ls_isna_prob, y_array_ls_isna_prob, sobol_filter_groups, ls_isna_model_stem\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packing Fraction Models\n",
    "### fba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobol_reg_fba = sobol_reg.dropna(subset=[\"fba\"])\n",
    "X_array_fba = sobol_reg_fba[fba_features].to_numpy()\n",
    "y_array_fba = sobol_reg_fba[\"fba\"].to_numpy().ravel()\n",
    "\n",
    "## create group for sobol regression fba features\n",
    "sobol_reg_fba_group = np.array(\n",
    "    [\n",
    "        f\"{mu1},{mu2},{std1},{std2},{std3},{comp1},{comp2},{num_part},{safety_fac}\"\n",
    "        for mu1, mu2, std1, std2, std3, comp1, comp2, num_part, safety_fac in zip(\n",
    "            sobol_reg_fba[\"mu1_div_mu3\"],\n",
    "            sobol_reg_fba[\"mu2_div_mu3\"],\n",
    "            sobol_reg_fba[\"std1\"],\n",
    "            sobol_reg_fba[\"std2\"],\n",
    "            sobol_reg_fba[\"std3\"],\n",
    "            sobol_reg_fba[\"comp1\"],\n",
    "            sobol_reg_fba[\"comp2\"],\n",
    "            sobol_reg_fba[\"num_particles\"],\n",
    "            sobol_reg_fba[\"safety_factor\"],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "## GroupKFold split for cv; using groups\n",
    "fba_model_stem = path.join(cv_model_dir, \"sobol_reg_fba\")\n",
    "fba_results = rfr_group_mae(\n",
    "    X_array_fba, y_array_fba, sobol_reg_fba_group, fba_model_stem\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for sobol_reg_ls: 0.0173 +/- 0.0048\n"
     ]
    }
   ],
   "source": [
    "sobol_reg_ls = sobol_reg.dropna(subset=[\"ls\"])\n",
    "X_array_ls = sobol_reg_ls[ls_features].to_numpy()\n",
    "y_array_ls = sobol_reg_ls[\"ls\"].to_numpy().ravel()\n",
    "\n",
    "## create group for sobol regression ls features\n",
    "sobol_reg_ls_group = np.array(\n",
    "    [\n",
    "        f\"{mu1},{mu2},{std1},{std2},{std3},{comp1},{comp2},{num_part},{safety_fac}\"\n",
    "        for mu1, mu2, std1, std2, std3, comp1, comp2, num_part, safety_fac in zip(\n",
    "            sobol_reg_ls[\"mu1_div_mu3\"],\n",
    "            sobol_reg_ls[\"mu2_div_mu3\"],\n",
    "            sobol_reg_ls[\"std1\"],\n",
    "            sobol_reg_ls[\"std2\"],\n",
    "            sobol_reg_ls[\"std3\"],\n",
    "            sobol_reg_ls[\"comp1\"],\n",
    "            sobol_reg_ls[\"comp2\"],\n",
    "            sobol_reg_ls[\"num_particles\"],\n",
    "            sobol_reg_ls[\"safety_factor\"],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "## GroupKFold split for cv; using groups\n",
    "ls_model_path = path.join(cv_model_dir, \"sobol_reg_ls\")\n",
    "ls_results = rfr_group_mae(X_array_ls, y_array_ls, sobol_reg_ls_group, ls_model_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Models\n",
    "No NaNs in the time values.\n",
    "### fba_time_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for sobol_reg_fba_time_s: 0.0367 +/- 0.0107\n"
     ]
    }
   ],
   "source": [
    "## create fba_time_s dataframe to use for groups\n",
    "fba_time_s_df = sobol_reg[fba_time_s_features]\n",
    "\n",
    "X_array_fba_time_s = sobol_reg[fba_time_s_features].to_numpy()\n",
    "fba_time_s = sobol_reg[[\"fba_time_s\"]]\n",
    "y_array_fba_time_s = fba_time_s.to_numpy().ravel()\n",
    "\n",
    "\n",
    "##create groups for fba_time_s GroupKFOld split\n",
    "sobol_reg_fba_time_s_group = np.array(\n",
    "    [\n",
    "        f\"{mu1},{mu2},{std1},{std2},{std3},{comp1},{comp2},{num_part},{safety_fac}\"\n",
    "        for mu1, mu2, std1, std2, std3, comp1, comp2, num_part, safety_fac in zip(\n",
    "            fba_time_s_df[\"mu1_div_mu3\"],\n",
    "            fba_time_s_df[\"mu2_div_mu3\"],\n",
    "            fba_time_s_df[\"std1\"],\n",
    "            fba_time_s_df[\"std2\"],\n",
    "            fba_time_s_df[\"std3\"],\n",
    "            fba_time_s_df[\"comp1\"],\n",
    "            fba_time_s_df[\"comp2\"],\n",
    "            fba_time_s_df[\"num_particles\"],\n",
    "            fba_time_s_df[\"safety_factor\"],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "fba_time_s_model_stem = path.join(cv_model_dir, \"sobol_reg_fba_time_s\")\n",
    "fba_time_s_results = rfr_group_mae(\n",
    "    X_array_fba_time_s,\n",
    "    y_array_fba_time_s,\n",
    "    sobol_reg_fba_time_s_group,\n",
    "    fba_time_s_model_stem,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ls_time_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for sobol_reg_ls_time_s: 2.3358 +/- 0.4001\n"
     ]
    }
   ],
   "source": [
    "##create df for ls_time_s\n",
    "ls_time_s_df = sobol_reg[ls_time_s_features]\n",
    "\n",
    "##create arrays for model\n",
    "X_array_ls_time_s = sobol_reg[ls_time_s_features].to_numpy()\n",
    "ls_time_s = sobol_reg[[\"ls_time_s\"]]\n",
    "y_array_ls_time_s = ls_time_s.to_numpy().ravel()\n",
    "\n",
    "\n",
    "##create groups for fba_time_s GroupKFOld split\n",
    "sobol_reg_ls_time_s_group = np.array(\n",
    "    [\n",
    "        f\"{mu1},{mu2},{std1},{std2},{std3},{comp1},{comp2},{num_part},{safety_fac}\"\n",
    "        for mu1, mu2, std1, std2, std3, comp1, comp2, num_part, safety_fac in zip(\n",
    "            ls_time_s_df[\"mu1_div_mu3\"],\n",
    "            ls_time_s_df[\"mu2_div_mu3\"],\n",
    "            ls_time_s_df[\"std1\"],\n",
    "            ls_time_s_df[\"std2\"],\n",
    "            ls_time_s_df[\"std3\"],\n",
    "            ls_time_s_df[\"comp1\"],\n",
    "            ls_time_s_df[\"comp2\"],\n",
    "            ls_time_s_df[\"num_particles\"],\n",
    "            ls_time_s_df[\"safety_factor\"],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "ls_time_s_model_stem = path.join(cv_model_dir, \"sobol_reg_ls_time_s\")\n",
    "ls_time_s_results = rfr_group_mae(\n",
    "    X_array_ls_time_s,\n",
    "    y_array_ls_time_s,\n",
    "    sobol_reg_ls_time_s_group,\n",
    "    ls_time_s_model_stem,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder where is the data and what is it saving\n",
    "main_results = {\n",
    "    \"fba_isna_prob\": fba_isna_results,\n",
    "    \"ls_isna_prob\": ls_isna_results,\n",
    "    \"fba\": fba_results,\n",
    "    \"ls\": ls_results,\n",
    "    \"fba_time_s\": fba_time_s_results,\n",
    "    \"ls_time_s\": ls_time_s_results,\n",
    "}\n",
    "with open(path.join(data_dir, \"model_metadata.json\"), \"w\") as f:\n",
    "    json.dump(main_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = {\n",
    "    \"fba_isna_prob\": fba_isna_model_stem,\n",
    "    \"ls_isna_prob\": ls_isna_model_stem,\n",
    "    \"fba\": fba_model_stem,\n",
    "    \"ls\": ls_model_path,\n",
    "    \"fba_time_s\": fba_time_s_model_stem,\n",
    "    \"ls_time_s\": ls_time_s_model_stem,\n",
    "}\n",
    "\n",
    "for i in range(5):\n",
    "    models = {}\n",
    "    for key, model_path in model_paths.items():\n",
    "        models[key] = joblib.load(f\"{model_path}_{i}.pkl\")\n",
    "\n",
    "    with open(path.join(cv_model_dir, f\"cross_validation_models_{i}.pkl\"), \"wb\") as f:\n",
    "        joblib.dump(models, f, compress=3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production models (full training data)\n",
    "Six keys in the dictionary, each key is a value of a label, and its value pair is the trained model.\n",
    "This trained model is stored in the models folder with the pickle file name \"trained_model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save(\n",
    "    spf_feat_array,\n",
    "    sr_feat_array,\n",
    "    spf_labels_array,\n",
    "    sr_labels_array,\n",
    "    spf_label_names,\n",
    "    sr_label_names,\n",
    "):\n",
    "    models = {}\n",
    "\n",
    "    for X1, y1, name1 in zip(spf_feat_array, spf_labels_array, spf_label_names):\n",
    "        print(f\"X1 spf shape: {X1.shape}, Y1 spf shape: {y1.shape}\")\n",
    "        model = RandomForestRegressor(random_state=13)\n",
    "        model.fit(X1, y1)\n",
    "        models[name1] = model\n",
    "\n",
    "    for X2, y2, name2 in zip(sr_feat_array, sr_labels_array, sr_label_names):\n",
    "        print(f\"X2 sr shape: {X2.shape}, Y2 sr shape: {y2.shape}\")\n",
    "        model = RandomForestRegressor(random_state=13)\n",
    "        model.fit(X2, y2)\n",
    "        models[name2] = model\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 spf shape: (100, 9), Y1 spf shape: (100,)\n",
      "X1 spf shape: (100, 9), Y1 spf shape: (100,)\n",
      "X2 sr shape: (86, 10), Y2 sr shape: (86,)\n",
      "X2 sr shape: (60, 10), Y2 sr shape: (60,)\n",
      "X2 sr shape: (100, 10), Y2 sr shape: (100,)\n",
      "X2 sr shape: (100, 10), Y2 sr shape: (100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['..\\\\..\\\\models\\\\particle_packing\\\\dummy\\\\surrogate_models.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of x_arrays, y_arrays, and target_names\n",
    "sobol_prob_filter_arrays = [X_array_fba_isna_prob, X_array_ls_isna_prob]\n",
    "sobol_prob_filter_labels = [y_array_fba_isna_prob, y_array_ls_isna_prob]\n",
    "sobol_filter_target_names = [\"fba_isna_prob\", \"ls_isna_prob\"]\n",
    "\n",
    "# List of x_arrays, y_arrays, and target_names\n",
    "sobol_reg_x_arrays = [X_array_fba, X_array_ls, X_array_fba_time_s, X_array_ls_time_s]\n",
    "sobol_reg_labels = [y_array_fba, y_array_ls, y_array_fba_time_s, y_array_ls_time_s]\n",
    "sobol_reg_target_names = [\"fba\", \"ls\", \"fba_time_s\", \"ls_time_s\"]\n",
    "\n",
    "# Train and save the model on all the data\n",
    "models = train_and_save(\n",
    "    sobol_prob_filter_arrays,\n",
    "    sobol_reg_x_arrays,\n",
    "    sobol_prob_filter_labels,\n",
    "    sobol_reg_labels,\n",
    "    sobol_filter_target_names,\n",
    "    sobol_reg_target_names,\n",
    ")\n",
    "\n",
    "joblib.dump(models, path.join(model_dir, \"surrogate_models.pkl\"), compress=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Graveyard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------#\n",
    "\n",
    "# original kfold split for cv; not using groups\n",
    "\n",
    "# argument for rfr_mae, X_array, y_array, model_name to save model as .pkl\n",
    "# def rfr_mae(X_array, y_array, model_name_stem, random_state=13):\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "#     mae_scores = []\n",
    "#     y_preds = []\n",
    "#     y_trues = []\n",
    "#     for i, (train_index, test_index) in enumerate(kf.split(X_array)):\n",
    "#         X_train, X_test = X_array[train_index], X_array[test_index]\n",
    "#         y_train, y_test = y_array[train_index], y_array[test_index]\n",
    "#         y_test = y_test.tolist()\n",
    "\n",
    "#         model = RandomForestRegressor(random_state=random_state)\n",
    "#         model.fit(X_train, y_train)\n",
    "#         y_pred = model.predict(X_test).tolist()\n",
    "\n",
    "#         y_preds.append(y_pred)\n",
    "#         y_trues.append(y_test)\n",
    "#         mae = mean_absolute_error(y_test, y_pred)\n",
    "#         mae_scores.append(mae)\n",
    "#         # save model as .pkl with compression\n",
    "#         # https://stackoverflow.com/a/47062881/13697228\n",
    "#         joblib.dump(model, f\"{model_name_stem}_{i}.pkl\", compress=3)\n",
    "#     avg_mae = np.mean(mae_scores)\n",
    "#     std_mae = np.std(mae_scores)\n",
    "#     print(f\"MAE for {path.basename(model_name_stem)}: {avg_mae:.4f} +/- {std_mae:.4f}\")\n",
    "#     results = {\"mae\": mae_scores, \"y_pred\": y_preds, \"y_true\": y_trues}\n",
    "#     return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KFold split for cv; not using groups\n",
    "# fba_isna_model_stem = path.join(cv_model_dir, \"spf_fba_isna_prob\")\n",
    "# fba_isna_results = rfr_mae(\n",
    "#     X_array_fba_isna_prob, y_array_fba_isna_prob,fba_isna_model_stem\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KFold split for cv; not using groups\n",
    "# fba_model_stem = path.join(cv_model_dir, \"sobol_reg_fba\")\n",
    "# fba_results = rfr_mae(X_array_fba, y_array_fba, fba_model_stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KFold split for cv; not using groups\n",
    "# ls_model_path = path.join(cv_model_dir, \"sobol_reg_ls\")\n",
    "# ls_results = rfr_mae(X_array_ls, y_array_ls, ls_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Sobol_reg feature and target lists\n",
    "# sobol_reg_features_lst = ['mu1_div_mu3', 'mu2_div_mu3', 'std1', 'std2', 'std3', 'comp1', 'comp2', 'num_particles', 'safety_factor', 'fba_rank', 'ls_rank', 'fba_time_s_rank', 'ls_time_s_rank']\n",
    "# sobol_reg_target_lst = ['fba', 'ls', 'fba_time_s', 'ls_time_s']\n",
    "\n",
    "# #Sobol_filter feature and target lists\n",
    "# sobol_filter_features_lst = ['mu1_div_mu3', 'mu2_div_mu3', 'std1', 'std2', 'std3', 'comp1', 'comp2', 'num_particles', 'safety_factor']\n",
    "# sobol_filter_target_lst = ['fba_isna_prob', 'ls_isna_prob']\n",
    "\n",
    "\n",
    "# def train_and_save(df1, df2,feature_lst_df1,feature_lst_df2,label_lst_df1, label_lst_df2):\n",
    "#     models = {}\n",
    "\n",
    "#     for label in label_lst_df1:\n",
    "#         X = df1[feature_lst_df1]\n",
    "#         y = df1[label]\n",
    "\n",
    "#         model = RandomForestRegressor(random_state=13)\n",
    "#         model.fit(X, y)\n",
    "#         model.predict(X)\n",
    "\n",
    "#         models[label] = model\n",
    "\n",
    "#     for label in label_lst_df2:\n",
    "#         X = df2[feature_lst_df2]\n",
    "#         y = df2[label]\n",
    "\n",
    "#         model = RandomForestRegressor(random_state=13)\n",
    "#         model.fit(X, y)\n",
    "\n",
    "#         models[label] = model\n",
    "\n",
    "#     joblib.dump(models, \"trained_model.pkl\")\n",
    "#     return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of arrays of x_arrays and y_arrays\n",
    "# X_arrays = [\n",
    "#     X_array_fba_isna_prob,\n",
    "#     X_array_ls_isna_prob,\n",
    "#     X_array_fba,\n",
    "#     X_array_ls,\n",
    "#     X_array_ls_time_s,\n",
    "#     X_array_ls_time_s,\n",
    "# ]\n",
    "# y_arrays = [\n",
    "#     y_array_fba_isna_prob,\n",
    "#     y_array_ls_isna_prob,\n",
    "#     y_array_fba,\n",
    "#     y_array_ls,\n",
    "#     y_array_fba_time_s,\n",
    "#     y_array_ls_time_s,\n",
    "# ]\n",
    "\n",
    "\n",
    "# rf = RandomForestRegressor(random_state=13)\n",
    "# trained_model_fba_isna_prob = rf.fit(X_array_fba_isna_prob, y_array_fba_isna_prob)\n",
    "# with open(path.join(model_dir, \"trained_model_fba_isna_prob.pkl\"), \"wb\") as f:\n",
    "#     joblib.dump(trained_model_fba_isna_prob, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Average MAE for fba_isna_prob\",rfr_mae(X_array_fba_isna_prob, y_array_fba_isna_prob,'fba_isna_prob.pkl'))\n",
    "\n",
    "# load trained model\n",
    "# loaded_model = joblib.load('fba_isna_prob_model.pkl')\n",
    "\n",
    "# Save the model\n",
    "# with open('../models/fba_isna_prob.pkl', 'wb') as f:\n",
    "#     pickle.dump(model, f)\n",
    "\n",
    "# # Load the model\n",
    "# with open('path/to/save/model.pkl', 'rb') as f:\n",
    "#     loaded_model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_sobol_filter = \"https://zenodo.org/record/7513019/files/sobol_probability_filter.csv\"\n",
    "# sobol_filter = pd.read_csv(url_sobol_filter)\n",
    "\n",
    "# url_sobol_reg = \"https://zenodo.org/record/7513019/files/sobol_regression.csv\"\n",
    "# sobol_reg = pd.read_csv(url_sobol_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getcwd()\n",
    "# os.chdir(\"../data/raw\")\n",
    "\n",
    "# sobol_filter.to_csv('sobol_filter.csv', index=False)\n",
    "\n",
    "# sobol_reg.to_csv('sobol_reg.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sobol_regression.csv\n",
    "# url_sobol_reg = \"https://zenodo.org/record/7513019/files/sobol_regression.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sobol_reg_x = sobol_reg[\n",
    "#     [\n",
    "#         \"mu1_div_mu3\",\n",
    "#         \"mu2_div_mu3\",\n",
    "#         \"std1\",\n",
    "#         \"std2\",\n",
    "#         \"std3\",\n",
    "#         \"comp1\",\n",
    "#         \"comp2\",\n",
    "#         \"num_particles\",\n",
    "#         \"safety_factor\",\n",
    "#         \"fba_rank\",\n",
    "#         \"ls_rank\",\n",
    "#         \"fba_time_s_rank\",\n",
    "#         \"ls_time_s_rank\",\n",
    "#     ]\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(sobol_reg_x))\n",
    "# print(len(fba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     \"Average MAE for ls_time_s\",\n",
    "#     rfr_mae(X_array_fba_time_s, y_array_ls_time_s, \"sobol_reg_ls_time_s.pkl\"),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parse data for target \"fba_isna_prob\"\n",
    "# fba_isna_prob = sobol_filter[\"fba_isna_prob\"]\n",
    "# sobolPF_fba_isna_prob = sobol_filter.drop([\"ls_isna_prob\", \"fba_isna_prob\"], axis=1)\n",
    "# fba_isna_prob = fba_isna_prob.to_frame()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matsci-opt-benchmarks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01883adffc5ff99e80740fdb2688c7d7f1b5220f2274814f600fbe3b3887f376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
