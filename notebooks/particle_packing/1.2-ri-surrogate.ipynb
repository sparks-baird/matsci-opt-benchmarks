{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Packing Surrogate Model\n",
    "\n",
    "Here, we train a surrogate model for the particle packing simulations. We capture the\n",
    "presence of failed simulations, the packing fractions for two different algorithms, and\n",
    "the corresponding runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from os import path\n",
    "import json\n",
    "\n",
    "# attempted use of skl2onnx to convert to onnx failing due to protobuf error\n",
    "# https://github.com/onnx/onnx/issues/4469\n",
    "\n",
    "# from skl2onnx import convert_sklearn\n",
    "# from skl2onnx.common.data_types import FloatTensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = False\n",
    "\n",
    "data_dir = path.join(\"..\", \"..\", \"data\", \"processed\", \"particle_packing\")\n",
    "model_dir = path.join(\"..\", \"..\", \"models\", \"particle_packing\")\n",
    "\n",
    "if dummy:\n",
    "    model_dir = path.join(model_dir, \"dummy\")\n",
    "\n",
    "cv_model_dir = path.join(model_dir, \"cv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobol_filter = pd.read_csv(path.join(data_dir, \"sobol_probability_filter.csv\"))\n",
    "sobol_reg = pd.read_csv(path.join(data_dir, \"sobol_regression.csv\"))\n",
    "\n",
    "if dummy:\n",
    "    sobol_filter = sobol_filter.head(100)\n",
    "    sobol_reg = sobol_reg.head(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define f(x) to calc mae scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument for rfr_mae, X_array, y_array, model_name to save model as .pkl\n",
    "def rfr_mae(X_array, y_array, model_name_stem, random_state=13):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    mae_scores = []\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_array)):\n",
    "        X_train, X_test = X_array[train_index], X_array[test_index]\n",
    "        y_train, y_test = y_array[train_index], y_array[test_index]\n",
    "        y_test = y_test.tolist()\n",
    "\n",
    "        model = RandomForestRegressor(random_state=random_state)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test).tolist()\n",
    "\n",
    "        y_preds.append(y_pred)\n",
    "        y_trues.append(y_test)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mae_scores.append(mae)\n",
    "        # save model as .pkl with compression\n",
    "        # https://stackoverflow.com/a/47062881/13697228\n",
    "        joblib.dump(model, f\"{model_name_stem}_{i}.pkl\", compress=3)\n",
    "    avg_mae = np.mean(mae_scores)\n",
    "    std_mae = np.std(mae_scores)\n",
    "    print(f\"MAE for {path.basename(model_name_stem)}: {avg_mae:.4f} +/- {std_mae:.4f}\")\n",
    "    results = {\"mae\": mae_scores, \"y_pred\": y_preds, \"y_true\": y_trues}\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_features = [\n",
    "    \"mu1_div_mu3\",\n",
    "    \"mu2_div_mu3\",\n",
    "    \"std1\",\n",
    "    \"std2\",\n",
    "    \"std3\",\n",
    "    \"comp1\",\n",
    "    \"comp2\",\n",
    "    \"num_particles\",\n",
    "    \"safety_factor\",\n",
    "]\n",
    "\n",
    "fba_isna_prob_features = common_features\n",
    "ls_isna_prob_features = common_features\n",
    "fba_features = common_features + [\"fba_rank\"]\n",
    "ls_features = common_features + [\"ls_rank\"]\n",
    "fba_time_s_features = common_features + [\"fba_time_s_rank\"]\n",
    "ls_time_s_features = common_features + [\"ls_time_s_rank\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Filter\n",
    "### fba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for spf_fba_isna_prob: 0.0488 +/- 0.0008\n"
     ]
    }
   ],
   "source": [
    "# Create a KFold cross-validation iterator\n",
    "\n",
    "X_array_fba_isna_prob = sobol_filter[fba_isna_prob_features].to_numpy()\n",
    "y_array_fba_isna_prob = sobol_filter[[\"fba_isna_prob\"]].to_numpy().ravel()\n",
    "\n",
    "# This is the trained model on As a function of mu1_div_mu3, mu2_div_mu3, std1, std2,\n",
    "# std3, comp1, comp2, num_particles, safety_factor\n",
    "# label data = fba_isna_prob\n",
    "\n",
    "fba_isna_model_stem = path.join(cv_model_dir, \"spf_fba_isna_prob\")\n",
    "fba_isna_results = rfr_mae(\n",
    "    X_array_fba_isna_prob, y_array_fba_isna_prob, fba_isna_model_stem\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test loading the pickled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [0.15470296 0.         0.00153846 0.06294192 0.10488284]\n",
      "1: [0.15378261 0.00166667 0.01487962 0.06537587 0.1104861 ]\n",
      "2: [0.15700882 0.         0.00901515 0.02742585 0.10563564]\n",
      "3: [0.16072028 0.         0.0168209  0.06632731 0.14089788]\n",
      "4: [0.15331041 0.005      0.03568301 0.0650517  0.09046243]\n"
     ]
    }
   ],
   "source": [
    "test_data = X_array_fba_isna_prob[:5]\n",
    "for i in range(5):\n",
    "    model = joblib.load(f\"{fba_isna_model_stem}_{i}.pkl\")\n",
    "    print(f\"{i}: {model.predict(test_data)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for spf_ls_isna_prob: 0.0911 +/- 0.0017\n"
     ]
    }
   ],
   "source": [
    "sobolPF_ls_isna_prob = sobol_filter[ls_isna_prob_features]\n",
    "ls_isna_prob = sobol_filter[[\"ls_isna_prob\"]]\n",
    "\n",
    "X_array_ls_isna_prob = sobolPF_ls_isna_prob.to_numpy()\n",
    "y_array_ls_isna_prob = ls_isna_prob.to_numpy().ravel()\n",
    "\n",
    "ls_isna_model_stem = path.join(cv_model_dir, \"spf_ls_isna_prob\")\n",
    "ls_isna_results = rfr_mae(\n",
    "    X_array_ls_isna_prob, y_array_ls_isna_prob, ls_isna_model_stem\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packing Fraction Models\n",
    "### fba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for sobol_reg_fba: 0.0033 +/- 0.0000\n"
     ]
    }
   ],
   "source": [
    "sobol_reg_fba = sobol_reg.dropna(subset=[\"fba\"])\n",
    "X_array_fba = sobol_reg_fba[fba_features].to_numpy()\n",
    "y_array_fba = sobol_reg_fba[\"fba\"].to_numpy().ravel()\n",
    "\n",
    "fba_model_stem = path.join(cv_model_dir, \"sobol_reg_fba\")\n",
    "fba_results = rfr_mae(X_array_fba, y_array_fba, fba_model_stem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for sobol_reg_ls: 0.0063 +/- 0.0000\n"
     ]
    }
   ],
   "source": [
    "sobol_reg_ls = sobol_reg.dropna(subset=[\"ls\"])\n",
    "X_array_ls = sobol_reg_ls[ls_features].to_numpy()\n",
    "y_array_ls = sobol_reg_ls[\"ls\"].to_numpy().ravel()\n",
    "\n",
    "ls_model_path = path.join(cv_model_dir, \"sobol_reg_ls\")\n",
    "ls_results = rfr_mae(X_array_ls, y_array_ls, ls_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Models\n",
    "No NaNs in the time values.\n",
    "### fba_time_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for sobol_reg_fba_time_s: 0.0320 +/- 0.0004\n"
     ]
    }
   ],
   "source": [
    "X_array_fba_time_s = sobol_reg[fba_time_s_features].to_numpy()\n",
    "fba_time_s = sobol_reg[[\"fba_time_s\"]]\n",
    "y_array_fba_time_s = fba_time_s.to_numpy().ravel()\n",
    "\n",
    "fba_time_s_model_stem = path.join(cv_model_dir, \"sobol_reg_fba_time_s\")\n",
    "fba_time_s_results = rfr_mae(\n",
    "    X_array_fba_time_s, y_array_fba_time_s, fba_time_s_model_stem,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ls_time_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for sobol_reg_ls_time_s: 89.8419 +/- 2.7097\n"
     ]
    }
   ],
   "source": [
    "X_array_ls_time_s = sobol_reg[ls_time_s_features].to_numpy()\n",
    "ls_time_s = sobol_reg[[\"ls_time_s\"]]\n",
    "y_array_ls_time_s = ls_time_s.to_numpy().ravel()\n",
    "\n",
    "ls_time_s_model_stem = path.join(cv_model_dir, \"sobol_reg_ls_time_s\")\n",
    "ls_time_s_results = rfr_mae(X_array_ls_time_s, y_array_ls_time_s, ls_time_s_model_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder where is the data and what is it saving\n",
    "main_results = {\n",
    "    \"fba_isna_prob\": fba_isna_results,\n",
    "    \"ls_isna_prob\": ls_isna_results,\n",
    "    \"fba\": fba_results,\n",
    "    \"ls\": ls_results,\n",
    "    \"fba_time_s\": fba_time_s_results,\n",
    "    \"ls_time_s\": ls_time_s_results,\n",
    "}\n",
    "with open(path.join(data_dir, \"model_metadata.json\"), \"w\") as f:\n",
    "    json.dump(main_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = {\n",
    "    \"fba_isna_prob\": fba_isna_model_stem,\n",
    "    \"ls_isna_prob\": ls_isna_model_stem,\n",
    "    \"fba\": fba_model_stem,\n",
    "    \"ls\": ls_model_path,\n",
    "    \"fba_time_s\": fba_time_s_model_stem,\n",
    "    \"ls_time_s\": ls_time_s_model_stem,\n",
    "}\n",
    "\n",
    "for i in range(5):\n",
    "    models = {}\n",
    "    for key, model_path in model_paths.items():\n",
    "        models[key] = joblib.load(f\"{model_path}_{i}.pkl\")\n",
    "\n",
    "    with open(path.join(cv_model_dir, f\"cross_validation_models_{i}.pkl\"), \"wb\") as f:\n",
    "        joblib.dump(models, f, compress=3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production models (full training data)\n",
    "Six keys in the dictionary, each key is a value of a label, and its value pair is the trained model.\n",
    "This trained model is stored in the models folder with the pickle file name \"trained_model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save(\n",
    "    spf_feat_array,\n",
    "    sr_feat_array,\n",
    "    spf_labels_array,\n",
    "    sr_labels_array,\n",
    "    spf_label_names,\n",
    "    sr_label_names,\n",
    "):\n",
    "    models = {}\n",
    "\n",
    "    for X1, y1, name1 in zip(spf_feat_array, spf_labels_array, spf_label_names):\n",
    "        print(f\"X1 spf shape: {X1.shape}, Y1 spf shape: {y1.shape}\")\n",
    "        model = RandomForestRegressor(random_state=13)\n",
    "        model.fit(X1, y1)\n",
    "        models[name1] = model\n",
    "\n",
    "    for X2, y2, name2 in zip(sr_feat_array, sr_labels_array, sr_label_names):\n",
    "        print(f\"X2 sr shape: {X2.shape}, Y2 sr shape: {y2.shape}\")\n",
    "        model = RandomForestRegressor(random_state=13)\n",
    "        model.fit(X2, y2)\n",
    "        models[name2] = model\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 spf shape: (41228, 9), Y1 spf shape: (41228,)\n",
      "X1 spf shape: (41228, 9), Y1 spf shape: (41228,)\n",
      "X2 sr shape: (408504, 10), Y2 sr shape: (408504,)\n",
      "X2 sr shape: (338467, 10), Y2 sr shape: (338467,)\n",
      "X2 sr shape: (438371, 10), Y2 sr shape: (438371,)\n",
      "X2 sr shape: (438371, 10), Y2 sr shape: (438371,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['..\\\\..\\\\models\\\\particle_packing\\\\surrogate_models.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of x_arrays, y_arrays, and target_names\n",
    "sobol_prob_filter_arrays = [X_array_fba_isna_prob, X_array_ls_isna_prob]\n",
    "sobol_prob_filter_labels = [y_array_fba_isna_prob, y_array_ls_isna_prob]\n",
    "sobol_filter_target_names = [\"fba_isna_prob\", \"ls_isna_prob\"]\n",
    "\n",
    "# List of x_arrays, y_arrays, and target_names\n",
    "sobol_reg_x_arrays = [X_array_fba, X_array_ls, X_array_fba_time_s, X_array_ls_time_s]\n",
    "sobol_reg_labels = [y_array_fba, y_array_ls, y_array_fba_time_s, y_array_ls_time_s]\n",
    "sobol_reg_target_names = [\"fba\", \"ls\", \"fba_time_s\", \"ls_time_s\"]\n",
    "\n",
    "# Train and save the model on all the data\n",
    "models = train_and_save(\n",
    "    sobol_prob_filter_arrays,\n",
    "    sobol_reg_x_arrays,\n",
    "    sobol_prob_filter_labels,\n",
    "    sobol_reg_labels,\n",
    "    sobol_filter_target_names,\n",
    "    sobol_reg_target_names,\n",
    ")\n",
    "\n",
    "joblib.dump(models, path.join(model_dir, \"surrogate_models.pkl\"), compress=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Graveyard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Sobol_reg feature and target lists\n",
    "# sobol_reg_features_lst = ['mu1_div_mu3', 'mu2_div_mu3', 'std1', 'std2', 'std3', 'comp1', 'comp2', 'num_particles', 'safety_factor', 'fba_rank', 'ls_rank', 'fba_time_s_rank', 'ls_time_s_rank']\n",
    "# sobol_reg_target_lst = ['fba', 'ls', 'fba_time_s', 'ls_time_s']\n",
    "\n",
    "# #Sobol_filter feature and target lists\n",
    "# sobol_filter_features_lst = ['mu1_div_mu3', 'mu2_div_mu3', 'std1', 'std2', 'std3', 'comp1', 'comp2', 'num_particles', 'safety_factor']\n",
    "# sobol_filter_target_lst = ['fba_isna_prob', 'ls_isna_prob']\n",
    "\n",
    "\n",
    "\n",
    "# def train_and_save(df1, df2,feature_lst_df1,feature_lst_df2,label_lst_df1, label_lst_df2):\n",
    "#     models = {}\n",
    "    \n",
    "#     for label in label_lst_df1:\n",
    "#         X = df1[feature_lst_df1]\n",
    "#         y = df1[label]\n",
    "        \n",
    "#         model = RandomForestRegressor(random_state=13)\n",
    "#         model.fit(X, y)\n",
    "#         model.predict(X)\n",
    "        \n",
    "#         models[label] = model\n",
    "        \n",
    "#     for label in label_lst_df2:\n",
    "#         X = df2[feature_lst_df2]\n",
    "#         y = df2[label]\n",
    "        \n",
    "#         model = RandomForestRegressor(random_state=13)\n",
    "#         model.fit(X, y)\n",
    "        \n",
    "#         models[label] = model\n",
    "        \n",
    "#     joblib.dump(models, \"trained_model.pkl\")    \n",
    "#     return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of arrays of x_arrays and y_arrays\n",
    "# X_arrays = [\n",
    "#     X_array_fba_isna_prob,\n",
    "#     X_array_ls_isna_prob,\n",
    "#     X_array_fba,\n",
    "#     X_array_ls,\n",
    "#     X_array_ls_time_s,\n",
    "#     X_array_ls_time_s,\n",
    "# ]\n",
    "# y_arrays = [\n",
    "#     y_array_fba_isna_prob,\n",
    "#     y_array_ls_isna_prob,\n",
    "#     y_array_fba,\n",
    "#     y_array_ls,\n",
    "#     y_array_fba_time_s,\n",
    "#     y_array_ls_time_s,\n",
    "# ]\n",
    "\n",
    "\n",
    "# rf = RandomForestRegressor(random_state=13)\n",
    "# trained_model_fba_isna_prob = rf.fit(X_array_fba_isna_prob, y_array_fba_isna_prob)\n",
    "# with open(path.join(model_dir, \"trained_model_fba_isna_prob.pkl\"), \"wb\") as f:\n",
    "#     joblib.dump(trained_model_fba_isna_prob, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Average MAE for fba_isna_prob\",rfr_mae(X_array_fba_isna_prob, y_array_fba_isna_prob,'fba_isna_prob.pkl'))\n",
    "\n",
    "# load trained model\n",
    "# loaded_model = joblib.load('fba_isna_prob_model.pkl')\n",
    "\n",
    "# Save the model\n",
    "# with open('../models/fba_isna_prob.pkl', 'wb') as f:\n",
    "#     pickle.dump(model, f)\n",
    "\n",
    "# # Load the model\n",
    "# with open('path/to/save/model.pkl', 'rb') as f:\n",
    "#     loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_sobol_filter = \"https://zenodo.org/record/7513019/files/sobol_probability_filter.csv\"\n",
    "# sobol_filter = pd.read_csv(url_sobol_filter)\n",
    "\n",
    "# url_sobol_reg = \"https://zenodo.org/record/7513019/files/sobol_regression.csv\"\n",
    "# sobol_reg = pd.read_csv(url_sobol_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getcwd()\n",
    "# os.chdir(\"../data/raw\")\n",
    "\n",
    "# sobol_filter.to_csv('sobol_filter.csv', index=False)\n",
    "\n",
    "# sobol_reg.to_csv('sobol_reg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sobol_regression.csv\n",
    "# url_sobol_reg = \"https://zenodo.org/record/7513019/files/sobol_regression.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sobol_reg_x = sobol_reg[\n",
    "#     [\n",
    "#         \"mu1_div_mu3\",\n",
    "#         \"mu2_div_mu3\",\n",
    "#         \"std1\",\n",
    "#         \"std2\",\n",
    "#         \"std3\",\n",
    "#         \"comp1\",\n",
    "#         \"comp2\",\n",
    "#         \"num_particles\",\n",
    "#         \"safety_factor\",\n",
    "#         \"fba_rank\",\n",
    "#         \"ls_rank\",\n",
    "#         \"fba_time_s_rank\",\n",
    "#         \"ls_time_s_rank\",\n",
    "#     ]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(sobol_reg_x))\n",
    "# print(len(fba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     \"Average MAE for ls_time_s\",\n",
    "#     rfr_mae(X_array_fba_time_s, y_array_ls_time_s, \"sobol_reg_ls_time_s.pkl\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parse data for target \"fba_isna_prob\"\n",
    "# fba_isna_prob = sobol_filter[\"fba_isna_prob\"]\n",
    "# sobolPF_fba_isna_prob = sobol_filter.drop([\"ls_isna_prob\", \"fba_isna_prob\"], axis=1)\n",
    "# fba_isna_prob = fba_isna_prob.to_frame()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matsci-opt-benchmarks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01883adffc5ff99e80740fdb2688c7d7f1b5220f2274814f600fbe3b3887f376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
